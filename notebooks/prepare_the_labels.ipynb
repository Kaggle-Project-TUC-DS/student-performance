{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory:  n:\\MASTER_DS\\Code\\Kaggle_competition\\Kaggle-seminar\\student-performance\n"
     ]
    }
   ],
   "source": [
    "#set working directory\n",
    "wd = os.path.dirname(os.getcwd())\n",
    "os.chdir(wd)\n",
    "print(\"Working Directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('data/raw/train_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['session'] = labels.session_id.apply(lambda x: int(x.split('_')[0]) )\n",
    "labels['q'] = labels.session_id.apply(lambda x: int(x.split('_')[-1][1:]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting the now obsolete labels of the labels\n",
    "labels= labels.set_index('session')\n",
    "labels = labels.drop([\"session_id\"], axis= 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mn:\\MASTER_DS\\Code\\Kaggle_competition\\Kaggle-seminar\\student-performance\\notebooks\\prepare_the_labels.ipynb Cell 6\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/prepare_the_labels.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#saving the dataset\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/prepare_the_labels.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m labels\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mdata/processed/labels.csv\u001b[39m\u001b[39m\"\u001b[39m,index_label\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msession\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "#saving the dataset\n",
    "\n",
    "\n",
    "\n",
    "labels.to_csv(\"data/processed/labels.csv\",index_label= \"session\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataframe labels is saved. \n",
    "it contains the cols:\n",
    " session : the session id\n",
    " correct : 0 and 1 for the answer of the question\n",
    " q: number of the question from 1 to 18\n",
    "\n",
    "The Data are stored as a .csv  \n",
    "\n",
    "to load the data efficiently use this loading function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes_labels= {\n",
    "    'correct': np.uint8, \n",
    "    'q':np.uint8}\n",
    "\n",
    "labels = pd.read_csv('data/processed/labels_q1-3.csv', dtype=dtypes_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(file_path: str , dtypes: dict = None, n_rows: int = None):\n",
    "    # If dtypes is not specified, set default data types for each column\n",
    "    if dtypes is None:\n",
    "        dtypes = {\n",
    "            'level': np.uint8,  \n",
    "            'session_id': np.int64,\n",
    "            'level_group': 'category',\n",
    "            'event_name': np.uint8,\n",
    "            'name': np.uint8,\n",
    "            'fqid': np.uint8,\n",
    "            'room_fqid': np.uint8,\n",
    "            'text_fqid': np.uint8,\n",
    "            'fullscreen': np.uint8,\n",
    "            'hq': np.uint8,\n",
    "            'music': np.uint8,\n",
    "            'hover_duration_mean': np.float32,\n",
    "            'difference_clicks_mean': np.float32,\n",
    "            \"distance_clicks_mean\": np.float32,\n",
    "            \"screen_distance_clicks_mean\": np.float32,            \n",
    "            'elapsed_time_std': np.float32,\n",
    "            'page_std': np.float32,\n",
    "            'room_coor_x_std': np.float32,\n",
    "            'room_coor_y_std': np.float32,\n",
    "            'screen_coor_x_std': np.float32,\n",
    "            'screen_coor_y_std': np.float32,\n",
    "            'hover_duration_std': np.float32,\n",
    "            'difference_clicks_std': np.float32,\n",
    "            \"distance_clicks_std\": np.float32,\n",
    "            \"screen_distance_clicks_std\": np.float32,\n",
    "            'index_sum_of_actions': np.int32,\n",
    "            'difference_clicks_max': np.float32,\n",
    "            'elapsed_time_max': np.float32,\n",
    "            'clicks_per_second': np.float32,\n",
    "            \"sum_distance_clicks_max\": np.float32,\n",
    "        }\n",
    "        \n",
    "    # Read in the CSV file\n",
    "    if n_rows is None:\n",
    "        df = pd.read_csv(file_path, dtype=dtypes, index_col = 0)\n",
    "    else:\n",
    "        df = pd.read_csv(file_path, dtype=dtypes, nrows=n_rows, index_col= 0)\n",
    "    \n",
    "    # Set data types for columns with \"_i\" index in their name\n",
    "    row, cols = df.shape\n",
    "    if cols > 50:\n",
    "        for column in df.columns:\n",
    "            base_name = column.rsplit('_', 1)[0]  # get the base name by splitting on the last \"_\" character\n",
    "            if base_name in dtypes:\n",
    "                column_number = column.rsplit('_', 1)[1]  # get the number from the index by splitting on the last \"_\" character\n",
    "                new_column_name = f\"{base_name}_{column_number}\"  # construct the new column name\n",
    "                column_dtype = dtypes[base_name]\n",
    "                df[new_column_name] = df[column].astype(column_dtype)  # set the same data type for all columns with the same base name\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_train_data(\"data/processed/df_0_4_flattened.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_9160\\2564891973.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels_q1.rename(columns= {\"session\": \"session_id_1\"}, inplace= True)\n"
     ]
    }
   ],
   "source": [
    "labels_q1 = labels[labels[\"q\"] == 1]\n",
    "labels_q1.rename(columns= {\"session\": \"session_id_1\"}, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(labels_q1, train_data, on='session_id_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = merged_df[\"correct\"]\n",
    "X = merged_df.drop(columns=[\"session_id_1\", \"level_group_1\", \"q\", \"correct\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "856/856 [==============================] - 2s 2ms/step - loss: 39200359448576.0000 - accuracy: 0.4972\n",
      "Epoch 2/10\n",
      "856/856 [==============================] - 2s 2ms/step - loss: 69312839680.0000 - accuracy: 0.4967\n",
      "Epoch 3/10\n",
      "856/856 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4939\n",
      "Epoch 4/10\n",
      "856/856 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4933\n",
      "Epoch 5/10\n",
      "856/856 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4930\n",
      "Epoch 6/10\n",
      "856/856 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4954\n",
      "Epoch 7/10\n",
      "856/856 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4949\n",
      "Epoch 8/10\n",
      "856/856 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "Epoch 9/10\n",
      "856/856 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4985\n",
      "Epoch 10/10\n",
      "856/856 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4959\n",
      "148/148 [==============================] - 0s 866us/step\n",
      "F1 Score: 0.8464080283930975\n",
      "Precision: 0.7337152556757903\n",
      "Recall: 1.0\n",
      "Confusion Matrix:\n",
      "[[   0 1255]\n",
      " [   0 3458]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have a DataFrame of features called \"df_X\" (shape: [n_samples, 107]) and\n",
    "# a target column called \"df_y\" (shape: [n_samples]).\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE to oversample the minority class (0)\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Define the MLP architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(X.shape[1],)))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "model.fit(X_train_resampled, y_train_resampled, epochs=epochs, batch_size=batch_size, class_weight=class_weights_dict)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print('F1 Score:', f1)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4, 1: 4709}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_pred, return_counts=  True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " ...\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8287642926022869\n",
      "Precision: 0.7913706919231781\n",
      "Recall: 0.869866975130133\n",
      "Confusion Matrix:\n",
      "[[ 462  793]\n",
      " [ 450 3008]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Assuming you have a DataFrame of features called \"X\" (shape: [n_samples, 107]) and\n",
    "# a target column called \"df_y\" (shape: [n_samples]).\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE to oversample the minority class (0)\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define the Random Forest classifier\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid)\n",
    "\n",
    "# Train the model using the grid search\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the testing set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print('F1 Score:', f1)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/steves_first_decisiontree_best_model_q1.joblib']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Save the best model to a file\n",
    "dump(best_model, 'models/steves_first_decisiontree_best_model_q1.joblib')\n",
    "\n",
    "# Load the best model from a file\n",
    "#best_model = load('models/steves_first_decisiontree_best_model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
