{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports for the Data Preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory:  n:\\MASTER_DS\\Code\\Kaggle_competition\\Kaggle-seminar\\student-performance\n"
     ]
    }
   ],
   "source": [
    "# get working directory and remove last folder\n",
    "# TODO: make this more robust\n",
    "wd = os.path.dirname(os.getcwd())\n",
    "os.chdir(wd)\n",
    "print('Working Directory: ', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes={\n",
    "    'elapsed_time':np.int32,\n",
    "    'event_name':'category',\n",
    "    'name':'category',\n",
    "    'level':np.uint8,\n",
    "    'room_coor_x':np.float32,\n",
    "    'index':np.int32,\n",
    "    'room_coor_y':np.float32,\n",
    "    'screen_coor_x':np.float32,\n",
    "    'screen_coor_y':np.float32,\n",
    "    'hover_duration':np.float32,\n",
    "    'text':'category',\n",
    "    'fqid':'category',\n",
    "    'room_fqid':'category',\n",
    "    'text_fqid':'category',\n",
    "    'fullscreen':'category',\n",
    "    'hq':'category',\n",
    "    'music':'category',\n",
    "    'level_group':'category'}\n",
    "\n",
    "\n",
    "# os.chdir('N:\\MASTER_DS\\Code\\Kaggle_competition\\Kaggle-seminar\\student-performance')\n",
    "dataset_df = pd.read_csv('data/raw/train.csv', dtype=dtypes, nrows= 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing function to add variables and rescale the time \n",
    "def adding_new_variables_rescaling(dataset_df):\n",
    "    dataset_df = dataset_df.sort_values(['session_id','elapsed_time'])\n",
    "    dataset_df['elapsed_time'] = dataset_df['elapsed_time']/1000\n",
    "    group = dataset_df.groupby(['session_id','level'])['elapsed_time'].diff()\n",
    "    group = group.fillna(value= 0)\n",
    "    dataset_df= dataset_df.assign(difference_clicks = group)\n",
    "\n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing to combine variables on level stage\n",
    "#Function to clean the sequential data for the training of the model\n",
    "\n",
    "#For that Function to work we need to specify the variables in Categorical and Numerical & Counting\n",
    "\n",
    "CATEGORICAL = ['event_name', 'name', 'fqid', 'room_fqid', 'text_fqid', 'fullscreen', 'hq', 'music']\n",
    "NUMERICALmean = ['hover_duration','difference_clicks']\n",
    "NUMERICALstd = ['elapsed_time','page','room_coor_x', 'room_coor_y', 'screen_coor_x', 'screen_coor_y', 'hover_duration', 'difference_clicks']\n",
    "COUNTING = ['index']\n",
    "MAXIMUM = ['difference_clicks', 'elapsed_time']\n",
    "\n",
    "\n",
    "\n",
    "def feature_engineer_steve(dataset_df):\n",
    "    dfs = []\n",
    "    tmp = dataset_df.groupby(['session_id','level'])[\"level_group\"].first()\n",
    "    tmp.name = tmp.name \n",
    "    dfs.append(tmp)\n",
    "    for c in CATEGORICAL:\n",
    "        if c not in ['fullscreen', 'hq', 'music']:\n",
    "            tmp = dataset_df.groupby(['session_id','level'])[c].agg('nunique')\n",
    "        else:\n",
    "            tmp = dataset_df.groupby(['session_id','level'])[c].first().astype(int).fillna(0)\n",
    "        dfs.append(tmp)\n",
    "    for c in NUMERICALmean:\n",
    "        tmp = dataset_df.groupby(['session_id','level'])[c].agg('mean')\n",
    "        tmp.name = tmp.name + '_mean'\n",
    "        dfs.append(tmp)\n",
    "    for c in NUMERICALstd:\n",
    "        tmp = dataset_df.groupby(['session_id','level'])[c].agg('std')\n",
    "        tmp.name = tmp.name + '_std'\n",
    "        dfs.append(tmp)    \n",
    "    for c in COUNTING:\n",
    "        tmp = 1+ dataset_df.groupby(['session_id','level'])[c].agg('max')- dataset_df.groupby(['session_id','level'])[c].agg('min') \n",
    "        tmp.name = tmp.name + '_sum_of_actions'\n",
    "        dfs.append(tmp)\n",
    "    for c in MAXIMUM:\n",
    "        tmp = dataset_df.groupby(['session_id','level'])[c].agg('max')- dataset_df.groupby(['session_id','level'])[c].agg('min') \n",
    "        tmp.name = tmp.name + '_max'\n",
    "        dfs.append(tmp)\n",
    "    \n",
    "    dataset_df = pd.concat(dfs,axis=1)\n",
    "    dataset_df = dataset_df.fillna(-1)\n",
    "    dataset_df = dataset_df.reset_index()\n",
    "    dataset_df = dataset_df.set_index('session_id') \n",
    "    \n",
    "# add Clicks per second afterwards cause we need the time for each level first\n",
    "    dataset_df['clicks_per_second'] = dataset_df['index_sum_of_actions']/ dataset_df['elapsed_time_max']\n",
    " \n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "\n",
    "def clear_memory(keep_list=None):\n",
    "    if keep_list is None:\n",
    "        keep_list = []\n",
    "    for name in dir():\n",
    "        if not name.startswith('_') and name not in keep_list:\n",
    "            value = globals()[name]\n",
    "            if isinstance(value, pd.DataFrame):\n",
    "                del globals()[name]\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              session_id  index  elapsed_time          event_name       name  \\\n",
      "0      20090312431273200      0         0.000      cutscene_click      basic   \n",
      "2      20090312431273200      2         0.831        person_click      basic   \n",
      "3      20090312431273200      3         1.147        person_click      basic   \n",
      "1      20090312431273200      1         1.323        person_click      basic   \n",
      "4      20090312431273200      4         1.863        person_click      basic   \n",
      "...                  ...    ...           ...                 ...        ...   \n",
      "19995  20090317080721164   1364      2454.048  notification_click      basic   \n",
      "19996  20090317080721164   1365      2455.479        object_click      basic   \n",
      "19997  20090317080721164   1366      2456.860        object_hover  undefined   \n",
      "19998  20090317080721164   1367      2457.726        object_click      basic   \n",
      "19999  20090317080721164   1368      2459.226  notification_click      basic   \n",
      "\n",
      "       level  page  room_coor_x  room_coor_y  screen_coor_x  ...  \\\n",
      "0          0   NaN  -413.991394  -159.314682          380.0  ...   \n",
      "2          0   NaN  -413.991394  -159.314682          380.0  ...   \n",
      "3          0   NaN  -413.991394  -159.314682          380.0  ...   \n",
      "1          0   NaN  -413.991394  -159.314682          380.0  ...   \n",
      "4          0   NaN  -412.991394  -159.314682          381.0  ...   \n",
      "...      ...   ...          ...          ...            ...  ...   \n",
      "19995     21   NaN   406.725525   -73.244423          771.0  ...   \n",
      "19996     21   NaN   406.725525   -73.244423          771.0  ...   \n",
      "19997     21   NaN          NaN          NaN            NaN  ...   \n",
      "19998     21   NaN  -101.274475    15.755580          263.0  ...   \n",
      "19999     21   NaN   423.725525   -83.244423          788.0  ...   \n",
      "\n",
      "       hover_duration                                         text  \\\n",
      "0                 NaN                                    undefined   \n",
      "2                 NaN                       Just talking to Teddy.   \n",
      "3                 NaN                   I gotta run to my meeting!   \n",
      "1                 NaN                Whatcha doing over there, Jo?   \n",
      "4                 NaN                          Can I come, Gramps?   \n",
      "...               ...                                          ...   \n",
      "19995             NaN  I should go to the Capitol and tell Mrs. M!   \n",
      "19996             NaN                                          NaN   \n",
      "19997          7826.0                                          NaN   \n",
      "19998             NaN                                          NaN   \n",
      "19999             NaN                 Look at all those activists!   \n",
      "\n",
      "                            fqid                       room_fqid  \\\n",
      "0                          intro  tunic.historicalsociety.closet   \n",
      "2                         gramps  tunic.historicalsociety.closet   \n",
      "3                         gramps  tunic.historicalsociety.closet   \n",
      "1                         gramps  tunic.historicalsociety.closet   \n",
      "4                         gramps  tunic.historicalsociety.closet   \n",
      "...                          ...                             ...   \n",
      "19995                        NaN  tunic.historicalsociety.stacks   \n",
      "19996   journals_flag.pic_2.next  tunic.historicalsociety.stacks   \n",
      "19997  journals_flag.pic_2.bingo  tunic.historicalsociety.stacks   \n",
      "19998  journals_flag.pic_0.bingo  tunic.historicalsociety.stacks   \n",
      "19999                        NaN  tunic.historicalsociety.stacks   \n",
      "\n",
      "                                               text_fqid fullscreen hq music  \\\n",
      "0                   tunic.historicalsociety.closet.intro          0  0     1   \n",
      "2      tunic.historicalsociety.closet.gramps.intro_0_...          0  0     1   \n",
      "3      tunic.historicalsociety.closet.gramps.intro_0_...          0  0     1   \n",
      "1      tunic.historicalsociety.closet.gramps.intro_0_...          0  0     1   \n",
      "4      tunic.historicalsociety.closet.gramps.intro_0_...          0  0     1   \n",
      "...                                                  ...        ... ..   ...   \n",
      "19995  tunic.historicalsociety.stacks.journals_flag.p...          0  0     1   \n",
      "19996                                                NaN          0  0     1   \n",
      "19997                                                NaN          0  0     1   \n",
      "19998                                                NaN          0  0     1   \n",
      "19999  tunic.historicalsociety.stacks.journals_flag.p...          0  0     1   \n",
      "\n",
      "      level_group difference_clicks  \n",
      "0             0-4             0.000  \n",
      "2             0-4             0.831  \n",
      "3             0-4             0.316  \n",
      "1             0-4             0.176  \n",
      "4             0-4             0.540  \n",
      "...           ...               ...  \n",
      "19995       13-22             0.808  \n",
      "19996       13-22             1.431  \n",
      "19997       13-22             1.381  \n",
      "19998       13-22             0.866  \n",
      "19999       13-22             1.500  \n",
      "\n",
      "[20000 rows x 21 columns]\n",
      "                   level level_group  event_name  name  fqid  room_fqid  \\\n",
      "session_id                                                                \n",
      "20090312431273200      0         0-4           6     3     5          1   \n",
      "20090312431273200      1         0-4           6     3     6          3   \n",
      "20090312431273200      2         0-4           6     3     8          2   \n",
      "20090312431273200      3         0-4           9     3    13          6   \n",
      "20090312431273200      4         0-4           4     2     5          2   \n",
      "...                  ...         ...         ...   ...   ...        ...   \n",
      "20090317080721164     17       13-22           4     4     5          4   \n",
      "20090317080721164     18       13-22          10     4    17          3   \n",
      "20090317080721164     19       13-22           7     4     9          2   \n",
      "20090317080721164     20       13-22           7     3    14          3   \n",
      "20090317080721164     21       13-22           8     4    15          5   \n",
      "\n",
      "                   text_fqid  fullscreen  hq  music  ...  room_coor_x_std  \\\n",
      "session_id                                           ...                    \n",
      "20090312431273200          6           0   0      1  ...       469.524328   \n",
      "20090312431273200          3           0   0      1  ...       358.166662   \n",
      "20090312431273200          5           0   0      1  ...       324.386196   \n",
      "20090312431273200          5           0   0      1  ...       389.874453   \n",
      "20090312431273200          0           0   0      1  ...       422.014525   \n",
      "...                      ...         ...  ..    ...  ...              ...   \n",
      "20090317080721164          2           0   0      1  ...       605.727392   \n",
      "20090317080721164         14           0   0      1  ...       539.819725   \n",
      "20090317080721164          5           0   0      1  ...       699.746370   \n",
      "20090317080721164          3           0   0      1  ...       277.277660   \n",
      "20090317080721164          4           0   0      1  ...       332.716483   \n",
      "\n",
      "                   room_coor_y_std  screen_coor_x_std  screen_coor_y_std  \\\n",
      "session_id                                                                 \n",
      "20090312431273200        84.862476         211.047234          87.147142   \n",
      "20090312431273200       116.538196         188.452544          89.020102   \n",
      "20090312431273200       158.503091         178.524529         106.028716   \n",
      "20090312431273200       117.002236         204.998931         106.129303   \n",
      "20090312431273200        41.143422         181.938451          39.566540   \n",
      "...                            ...                ...                ...   \n",
      "20090317080721164       190.298909         254.499702         126.715013   \n",
      "20090317080721164       278.060536         188.042548          94.995875   \n",
      "20090317080721164       334.139127         285.106399         152.887989   \n",
      "20090317080721164       121.906446         190.157612         114.305189   \n",
      "20090317080721164       126.627917         222.888578         112.787849   \n",
      "\n",
      "                   hover_duration_std  difference_clicks_std  \\\n",
      "session_id                                                     \n",
      "20090312431273200           -1.000000               0.473709   \n",
      "20090312431273200           -1.000000               1.068973   \n",
      "20090312431273200         2509.521966               0.520979   \n",
      "20090312431273200         3595.354271               0.886427   \n",
      "20090312431273200          164.755880               8.380692   \n",
      "...                               ...                    ...   \n",
      "20090317080721164           -1.000000               0.983913   \n",
      "20090317080721164         4256.798661               1.662691   \n",
      "20090317080721164          117.126712               1.301016   \n",
      "20090317080721164         3196.247205               1.395779   \n",
      "20090317080721164         2964.786606               1.445493   \n",
      "\n",
      "                   index_sum_of_actions  difference_clicks_max  \\\n",
      "session_id                                                       \n",
      "20090312431273200                    28                  1.784   \n",
      "20090312431273200                    32                  4.864   \n",
      "20090312431273200                    39                  2.215   \n",
      "20090312431273200                    53                  4.540   \n",
      "20090312431273200                    13                 30.837   \n",
      "...                                 ...                    ...   \n",
      "20090317080721164                    29                  4.382   \n",
      "20090317080721164                   135                 14.273   \n",
      "20090317080721164                    67                  6.045   \n",
      "20090317080721164                    49                  8.418   \n",
      "20090317080721164                    54                  8.560   \n",
      "\n",
      "                   elapsed_time_max  clicks_per_second  \n",
      "session_id                                              \n",
      "20090312431273200            25.766           1.086703  \n",
      "20090312431273200            37.346           0.856852  \n",
      "20090312431273200            34.253           1.138586  \n",
      "20090312431273200            54.712           0.968709  \n",
      "20090312431273200            38.621           0.336604  \n",
      "...                             ...                ...  \n",
      "20090317080721164            40.770           0.711307  \n",
      "20090317080721164           230.340           0.586090  \n",
      "20090317080721164            98.990           0.676836  \n",
      "20090317080721164            67.548           0.725410  \n",
      "20090317080721164            91.930           0.587403  \n",
      "\n",
      "[412 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "#return the preprocessed data\n",
    "##test data preprocessing with Subset of 5 million rows\n",
    "\n",
    "dataset_df_added = adding_new_variables_rescaling(dataset_df)\n",
    "dataset_df_level = feature_engineer_steve(dataset_df_added)\n",
    "print(dataset_df_added)\n",
    "print(dataset_df_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "#dataset_df_added.to_csv('data/processed/data_df_added.csv')\n",
    "#dataset_df_level.to_csv('data/processed/data_df_level.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level                       uint64\n",
      "level_group               category\n",
      "event_name                   int64\n",
      "name                         int64\n",
      "fqid                         int64\n",
      "room_fqid                    int64\n",
      "text_fqid                    int64\n",
      "fullscreen                   int32\n",
      "hq                           int32\n",
      "music                        int32\n",
      "hover_duration_mean        float32\n",
      "difference_clicks_mean     float64\n",
      "elapsed_time_std           float64\n",
      "page_std                   float64\n",
      "room_coor_x_std            float64\n",
      "room_coor_y_std            float64\n",
      "screen_coor_x_std          float64\n",
      "screen_coor_y_std          float64\n",
      "hover_duration_std         float64\n",
      "difference_clicks_std      float64\n",
      "index_sum_of_actions         int32\n",
      "difference_clicks_max      float64\n",
      "elapsed_time_max           float64\n",
      "clicks_per_second          float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dataset_df_level.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['level', 'level_group', 'event_name', 'name', 'fqid', 'room_fqid',\n",
       "       'text_fqid', 'fullscreen', 'hq', 'music', 'hover_duration_mean',\n",
       "       'difference_clicks_mean', 'elapsed_time_std', 'page_std',\n",
       "       'room_coor_x_std', 'room_coor_y_std', 'screen_coor_x_std',\n",
       "       'screen_coor_y_std', 'hover_duration_std', 'difference_clicks_std',\n",
       "       'index_sum_of_actions', 'difference_clicks_max', 'elapsed_time_max',\n",
       "       'clicks_per_second'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df_level.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level                        uint8\n",
      "level_group               category\n",
      "event_name                   uint8\n",
      "name                         uint8\n",
      "fqid                         uint8\n",
      "room_fqid                    uint8\n",
      "text_fqid                    uint8\n",
      "fullscreen                   uint8\n",
      "hq                           uint8\n",
      "music                        uint8\n",
      "hover_duration_mean          int32\n",
      "difference_clicks_mean       int32\n",
      "elapsed_time_std             int32\n",
      "page_std                     int32\n",
      "room_coor_x_std              int32\n",
      "room_coor_y_std              int32\n",
      "screen_coor_x_std            int32\n",
      "screen_coor_y_std            int32\n",
      "hover_duration_std           int32\n",
      "difference_clicks_std        int32\n",
      "index_sum_of_actions         int32\n",
      "difference_clicks_max        int32\n",
      "elapsed_time_max             int32\n",
      "clicks_per_second            int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#split the dataframe into three different ones depending on the level group\n",
    "dtypes = {\n",
    "    'level': np.uint8,\n",
    "    \"level_group\": \"category\",\n",
    "    'event_name': np.uint8,\n",
    "    'name': np.uint8,\n",
    "    'fqid': np.uint8,\n",
    "    'room_fqid': np.uint8,           \n",
    "    \"text_fqid\": np.uint8,\n",
    "    'fullscreen': np.uint8,\n",
    "    'hq': np.uint8,\n",
    "    'music': np.uint8,\n",
    "    'hover_duration_mean': np.int32,\n",
    "    'difference_clicks_mean': np.int32,\n",
    "    'elapsed_time_std': np.int32,\n",
    "    'page_std': np.int32,\n",
    "    'room_coor_x_std': np.int32,\n",
    "    'room_coor_y_std': np.int32,\n",
    "    'screen_coor_x_std': np.int32,\n",
    "    'screen_coor_y_std': np.int32,\n",
    "    'hover_duration_std': np.int32,\n",
    "    'difference_clicks_std': np.int32,\n",
    "    'index_sum_of_actions': np.int32,\n",
    "    'difference_clicks_max': np.int32,\n",
    "    'elapsed_time_max': np.int32,\n",
    "    'clicks_per_second': np.int32}\n",
    "groups = dataset_df_level.groupby('level_group')\n",
    "\n",
    "# Create a dictionary to store the resulting dataframes\n",
    "result = {}\n",
    "\n",
    "# Loop over each group\n",
    "for name, group in groups:\n",
    "    # Add the group to the result dictionary\n",
    "    result[name] = group.astype(dtypes)\n",
    "\n",
    "# Access the resulting dataframes using their keys\n",
    "df_0_4 = result['0-4']\n",
    "df_5_12 = result['5-12']\n",
    "df_13_22 = result['13-22']\n",
    "print(df_0_4.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'level': np.uint8,\n",
    "    \"level_group\": \"category\",\n",
    "    'event_name': np.uint8,\n",
    "    'name': np.uint8,\n",
    "    'fqid': np.uint8,\n",
    "    'room_fqid': np.uint8,           \n",
    "    \"text_fqid\": np.uint8,\n",
    "    'fullscreen': np.uint8,\n",
    "    'hq': np.uint8,\n",
    "    'music': np.uint8,\n",
    "    'hover_duration_mean': np.int32,\n",
    "    'difference_clicks_mean': np.int32,\n",
    "    'elapsed_time_std': np.int32,\n",
    "    'page_std': np.int32,\n",
    "    'room_coor_x_std': np.int32,\n",
    "    'room_coor_y_std': np.int32,\n",
    "    'screen_coor_x_std': np.int32,\n",
    "    'screen_coor_y_std': np.int32,\n",
    "    'hover_duration_std': np.int32,\n",
    "    'difference_clicks_std': np.int32,\n",
    "    'index_sum_of_actions': np.int32,\n",
    "    'difference_clicks_max': np.int32,\n",
    "    'elapsed_time_max': np.int32,\n",
    "    'clicks_per_second': np.int32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# sample data with NaN values\n",
    "df = pd.DataFrame({\n",
    "    'session_id': [1, 1, 2, 2],\n",
    "    'level_group': ['0-4', '0-4', '0-4', '0-4'],\n",
    "    'col1': [1, 2, np.nan, 4],\n",
    "    'col2': [5, 6, 7, 8],\n",
    "    'col3': [9, 10, 11, np.nan]\n",
    "})\n",
    "\n",
    "# get the unique session_ids and columns\n",
    "session_ids = df['session_id'].unique()\n",
    "cols = ['session_id'] + [f'{col}_{i+1}' for i in range(len(session_ids)) for col in ['col1', 'col2', 'col3']]\n",
    "\n",
    "# create a new dataframe to hold the flattened data\n",
    "new_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "# loop through each session_id\n",
    "for i, session_id in enumerate(session_ids):\n",
    "    df_session = df[df['session_id'] == session_id].reset_index(drop=True)\n",
    "    # combine the columns into a single row\n",
    "    row_data = [df_session[col].iloc[i] for i in range(len(df_session)) for col in ['col1', 'col2', 'col3']] \n",
    "    # add None values to the row if necessary to make it the same length as the columns\n",
    "    if len(row_data) < len(cols) - 1:\n",
    "        row_data += [None] * (len(cols) - len(row_data) - 1)\n",
    "    # add the session_id to the beginning of the row\n",
    "    row_data = [session_id] + row_data\n",
    "    # add the row to the new dataframe\n",
    "    new_df.loc[len(new_df)] = row_data\n",
    "\n",
    "# fill NaN values with np.nan\n",
    "new_df = new_df.fillna(np.nan)\n",
    "\n",
    "# convert the columns back to their original datatypes\n",
    "for col in ['col1', 'col2', 'col3']:\n",
    "    new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]] = new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]].astype(df[col].dtype, errors='ignore')\n",
    "\n",
    "# sort the columns\n",
    "new_df = new_df[cols]\n",
    "\n",
    "print(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putdated!!!!!!!!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def flatten_df(df):\n",
    "    # create a new index col\n",
    "    df = df.reset_index()\n",
    "    df['_index'] = df.index + 1\n",
    "    df = df.set_index('_index')\n",
    "    df = df.drop([\"level_group\"], axis= 1)\n",
    "\n",
    "    # get the unique session_ids and columns\n",
    "    session_ids = df['session_id'].unique()\n",
    "    cols = ['session_id'] + [f'{col}_{i+1}' for i in range(len(session_ids)) for col in df.columns if col != 'session_id']\n",
    "\n",
    "    # create a new dataframe to hold the flattened data\n",
    "    new_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    # loop through each session_id\n",
    "    for i, session_id in enumerate(session_ids):\n",
    "        df_session = df[df['session_id'] == session_id].reset_index(drop=True)\n",
    "        # combine the columns into a single row\n",
    "        row_data = [df_session[col].iloc[i] if pd.notnull(df_session[col].iloc[i]) else np.nan for i in range(len(df_session)) for col in df.columns if col != 'session_id'] \n",
    "        # add None values to the row if necessary to make it the same length as the columns\n",
    "        if len(row_data) < len(cols) - 1:\n",
    "            row_data += [np.nan] * (len(cols) - len(row_data) - 1)\n",
    "        # add the session_id to the beginning of the row\n",
    "        row_data = [session_id] + row_data\n",
    "        # add the row to the new dataframe\n",
    "        new_df.loc[len(new_df)] = row_data\n",
    "\n",
    "    # fill NaN values with np.nan\n",
    "    new_df = new_df.fillna(np.nan)\n",
    "\n",
    "    # convert the columns back to their original datatypes\n",
    "    for col in df.columns:\n",
    "        if col != 'session_id':\n",
    "            new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]] = new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]].astype(df[col].dtype, errors='ignore')\n",
    "\n",
    "    # sort the columns\n",
    "    new_df = new_df[cols]\n",
    "\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = flatten_df(df_5_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def flatten_df(df):\n",
    "    # create a new index col\n",
    "    df = df.reset_index()\n",
    "    df['_index'] = df.index + 1\n",
    "    df = df.set_index('_index')\n",
    "    df = df.drop([\"level_group\"], axis= 1)\n",
    "\n",
    "    # get the unique session_ids and columns\n",
    "    session_ids = df['session_id'].unique()\n",
    "    cols = ['session_id'] + [f'{col}_{i+1}' for i in range(len(session_ids)) for col in df.columns if col != 'session_id']\n",
    "\n",
    "    # create a new dataframe to hold the flattened data\n",
    "    new_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    # define a function to apply to each group\n",
    "    def flatten_group(group):\n",
    "        # combine the columns into a single row\n",
    "        row_data = [group[col].iloc[i] if pd.notnull(group[col].iloc[i]) else np.nan for i in range(len(group)) for col in group.columns if col != 'session_id'] \n",
    "        # add None values to the row if necessary to make it the same length as the columns\n",
    "        if len(row_data) < len(cols) - 1:\n",
    "            row_data += [np.nan] * (len(cols) - len(row_data) - 1)\n",
    "        # add the session_id to the beginning of the row\n",
    "        row_data = [group['session_id'].iloc[0]] + row_data\n",
    "        return pd.Series(row_data, index=cols)\n",
    "\n",
    "    # apply the function to each group and concatenate the results\n",
    "    new_df = pd.concat([flatten_group(group) for _, group in df.groupby('session_id')], axis=1).T\n",
    "\n",
    "    # fill NaN values with np.nan\n",
    "    new_df = new_df.fillna(np.nan)\n",
    "\n",
    "    # convert the columns back to their original datatypes\n",
    "    for col in df.columns:\n",
    "        if col != 'session_id':\n",
    "            new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]] = new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]].astype(df[col].dtype, errors='ignore')\n",
    "\n",
    "    # sort the columns\n",
    "    new_df = new_df[cols]\n",
    "\n",
    "    # remove columns that contain only NaN values\n",
    "    new_df = new_df.dropna(axis=1, how='all')\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def flatten_df(df):\n",
    "    # create a new index col\n",
    "    df = df.reset_index()\n",
    "    df['_index'] = df.index + 1\n",
    "    df = df.set_index('_index')\n",
    "    df = df.drop([\"level_group\"], axis= 1)\n",
    "\n",
    "    # get the unique session_ids and columns\n",
    "    session_ids = df['session_id'].unique()\n",
    "    cols = ['session_id'] + [f'{col}_{i+1}' for i in range(len(session_ids)) for col in df.columns if col != 'session_id']\n",
    "\n",
    "    # create a new dataframe to hold the flattened data\n",
    "    new_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    # define a function to apply to each group\n",
    "    def flatten_group(group):\n",
    "        # combine the columns into a single row\n",
    "        row_data = [group[col].iloc[i] if pd.notnull(group[col].iloc[i]) else np.nan for i in range(len(group)) for col in group.columns if col != 'session_id'] \n",
    "        # add None values to the row if necessary to make it the same length as the columns\n",
    "        if len(row_data) < len(cols) - 1:\n",
    "            row_data += [np.nan] * (len(cols) - len(row_data) - 1)\n",
    "        # add the session_id to the beginning of the row\n",
    "        row_data = [group['session_id'].iloc[0]] + row_data\n",
    "        return pd.Series(row_data, index=cols)\n",
    "\n",
    "    # apply the function to each group and concatenate the results\n",
    "    new_df = pd.concat([flatten_group(group) for _, group in df.groupby('session_id')], axis=1).T\n",
    "\n",
    "    # fill NaN values with np.nan\n",
    "    new_df = new_df.fillna(np.nan)\n",
    "\n",
    "    # convert the columns back to their original datatypes\n",
    "    for col in df.columns:\n",
    "        if col != 'session_id':\n",
    "            new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]] = new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]].astype(df[col].dtype, errors='ignore')\n",
    "\n",
    "    # sort the columns\n",
    "    new_df = new_df[cols]\n",
    "\n",
    "    # remove columns that contain only NaN values\n",
    "    new_df = new_df.dropna(axis=1, how='all')\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_4_flattened, df_5_12_flattened, df_13_22_flattened = flatten_df(dataset_df_level, exclude= [\"level\", \"music\",\"hq\",\"fullscreen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_4_flat = flatten_df(df_0_4, exclude= [\"level\", \"music\",\"hq\",\"fullscreen\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def flatten_df(df, exclude=[]):\n",
    "    # create a new index col\n",
    "    df = df.reset_index()\n",
    "    df['_index'] = df.index + 1\n",
    "    df = df.set_index('_index')\n",
    "    df = df.drop([\"level_group\"], axis= 1)\n",
    "\n",
    "    # get the unique session_ids and columns\n",
    "    session_ids = df['session_id'].unique()\n",
    "    cols = ['session_id'] + [f'{col}_{i+1}' for i in range(len(session_ids)) for col in df.columns if col != 'session_id']\n",
    "\n",
    "    # create a new dataframe to hold the flattened data\n",
    "    new_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    # define a function to apply to each group\n",
    "    def flatten_group(group):\n",
    "        # combine the columns into a single row\n",
    "        row_data = [group[col].iloc[i] if pd.notnull(group[col].iloc[i]) else np.nan for i in range(len(group)) for col in group.columns if col != 'session_id'] \n",
    "        # add None values to the row if necessary to make it the same length as the columns\n",
    "        if len(row_data) < len(cols) - 1:\n",
    "            row_data += [np.nan] * (len(cols) - len(row_data) - 1)\n",
    "        # add the session_id to the beginning of the row\n",
    "        row_data = [group['session_id'].iloc[0]] + row_data\n",
    "        return pd.Series(row_data, index=cols)\n",
    "\n",
    "    # apply the function to each group and concatenate the results\n",
    "    new_df = pd.concat([flatten_group(group) for _, group in df.groupby('session_id')], axis=1).T\n",
    "\n",
    "    # fill NaN values with np.nan\n",
    "    new_df = new_df.fillna(np.nan)\n",
    "\n",
    "    # convert the columns back to their original datatypes\n",
    "    for col in df.columns:\n",
    "        if col != 'session_id':\n",
    "            new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]] = new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]].astype(df[col].dtype, errors='ignore')\n",
    "\n",
    "    # sort the columns\n",
    "    new_df = new_df[cols]\n",
    "\n",
    "    # remove columns that contain only NaN values\n",
    "    new_df = new_df.dropna(axis=1, how='all')\n",
    "\n",
    "    # remove specified columns from exclude list\n",
    "    for col_name in exclude:\n",
    "        new_df = new_df[[col for col in new_df.columns if not (col.startswith(col_name + '_') and col != col_name + '_1')]]\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def flatten_df(dataset_df_level, exclude=[]):\n",
    "    #split the dataframe into three different ones depending on the level group\n",
    "    groups = dataset_df_level.groupby('level_group')\n",
    "\n",
    "    # Create a dictionary to store the resulting dataframes\n",
    "    result = {}\n",
    "\n",
    "    # Loop over each group\n",
    "    for name, group in groups:\n",
    "        # Add the group to the result dictionary\n",
    "        result[name] = group\n",
    "\n",
    "    # Access the resulting dataframes using their keys\n",
    "    df_0_4 = result['0-4']\n",
    "    df_5_12 = result['5-12']\n",
    "    df_13_22 = result['13-22']\n",
    "\n",
    "    dfs = [df_0_4, df_5_12, df_13_22]\n",
    "    flattened_dfs = []\n",
    "\n",
    "    for df in dfs:\n",
    "        # create a new index col\n",
    "        df = df.reset_index()\n",
    "        df['_index'] = df.index + 1\n",
    "        df = df.set_index('_index')\n",
    "        df = df.drop([\"level_group\"], axis= 1)\n",
    "\n",
    "        # get the unique session_ids and columns\n",
    "        session_ids = df['session_id'].unique()\n",
    "        cols = ['session_id'] + [f'{col}_{i+1}' for i in range(len(session_ids)) for col in df.columns if col != 'session_id']\n",
    "\n",
    "        # create a new dataframe to hold the flattened data\n",
    "        new_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "        # define a function to apply to each group\n",
    "        def flatten_group(group):\n",
    "            # combine the columns into a single row\n",
    "            row_data = [group[col].iloc[i] if pd.notnull(group[col].iloc[i]) else np.nan for i in range(len(group)) for col in group.columns if col != 'session_id'] \n",
    "            # add None values to the row if necessary to make it the same length as the columns\n",
    "            if len(row_data) < len(cols) - 1:\n",
    "                row_data += [np.nan] * (len(cols) - len(row_data) - 1)\n",
    "            # add the session_id to the beginning of the row\n",
    "            row_data = [group['session_id'].iloc[0]] + row_data\n",
    "            return pd.Series(row_data, index=cols)\n",
    "\n",
    "        # apply the function to each group and concatenate the results\n",
    "        new_df = pd.concat([flatten_group(group) for _, group in df.groupby('session_id')], axis=1).T\n",
    "\n",
    "        # fill NaN values with np.nan\n",
    "        new_df = new_df.fillna(np.nan)\n",
    "\n",
    "        # convert the columns back to their original datatypes\n",
    "        for col in df.columns:\n",
    "            if col != 'session_id':\n",
    "                new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]] = new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]].astype(df[col].dtype, errors='ignore')\n",
    "\n",
    "        # sort the columns\n",
    "        new_df = new_df[cols]\n",
    "\n",
    "        # remove columns that contain only NaN values\n",
    "        new_df = new_df.dropna(axis=1, how='all')\n",
    "\n",
    "        # remove specified columns from exclude list\n",
    "        for col_name in exclude:\n",
    "            new_df = new_df[[col for col in new_df.columns if not (col.startswith(col_name + '_') and col != col_name + '_1')]]\n",
    "\n",
    "        flattened_dfs.append(new_df)\n",
    "\n",
    "    return flattened_dfs[0], flattened_dfs[1], flattened_dfs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_df_one_at_a_time(df, exclude=[]):\n",
    "    # create a new index col\n",
    "    df = df.reset_index()\n",
    "    df['_index'] = df.index + 1\n",
    "    df = df.set_index('_index')\n",
    "    df = df.drop([\"level_group\"], axis=1)\n",
    "\n",
    "    # get the unique session_ids and columns\n",
    "    session_ids = df['session_id'].unique()\n",
    "    cols = ['session_id'] + [f'{col}_{i+1}' for i in range(len(session_ids)) for col in df.columns if col != 'session_id']\n",
    "\n",
    "    # create a new dataframe to hold the flattened data\n",
    "    new_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    # define a function to apply to each group\n",
    "    def flatten_group(group):\n",
    "        # combine the columns into a single row\n",
    "        row_data = [group[col].iloc[i] if pd.notnull(group[col].iloc[i]) else np.nan for i in range(len(group)) for col in group.columns if col != 'session_id']\n",
    "        # add None values to the row if necessary to make it the same length as the columns\n",
    "        if len(row_data) < len(cols) - 1:\n",
    "            row_data += [np.nan] * (len(cols) - len(row_data) - 1)\n",
    "        # add the session_id to the beginning of the row\n",
    "        row_data = [group['session_id'].iloc[0]] + row_data\n",
    "        return pd.Series(row_data, index=cols)\n",
    "\n",
    "    # apply the function to each group and concatenate the results\n",
    "    new_df = pd.concat([flatten_group(group) for _, group in df.groupby('session_id')], axis=1).T\n",
    "\n",
    "    # fill NaN values with np.nan\n",
    "    new_df = new_df.fillna(np.nan)\n",
    "\n",
    "    # convert the columns back to their original datatypes\n",
    "    for col in df.columns:\n",
    "        if col != 'session_id':\n",
    "            new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]] = new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]].astype(df[col].dtype, errors='ignore')\n",
    "\n",
    "    # sort the columns\n",
    "    new_df = new_df[cols]\n",
    "\n",
    "    # remove columns that contain only NaN values\n",
    "    new_df = new_df.dropna(axis=1, how='all')\n",
    "\n",
    "    # remove specified columns from exclude list\n",
    "    for col_name in exclude:\n",
    "        new_df = new_df[[col for col in new_df.columns if not (col.startswith(col_name + '_') and col != col_name + '_1')]]\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = [\"level\", \"music\",\"hq\",\"fullscreen\"]\n",
    "df_0_4 = flatten_df_one_at_a_time(df_0_4,exclude= ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_df_one_at_a_time(df, exclude=[]):\n",
    "    # create a new index col\n",
    "    df.reset_index(inplace=True)\n",
    "    df['_index'] = df.index + 1\n",
    "    df.set_index('_index', inplace=True)\n",
    "    \n",
    "    # drop the excluded columns\n",
    "    for col in exclude:\n",
    "        if col in df.columns:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "    # get the unique session_ids and columns\n",
    "    session_ids = df['session_id'].unique()\n",
    "    cols = ['session_id'] + [f'{col}_{i+1}' for i in range(len(session_ids)) for col in df.columns if col != 'session_id']\n",
    "\n",
    "    # create a new dataframe to hold the flattened data\n",
    "    new_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    # define a function to apply to each group\n",
    "    def flatten_group(group):\n",
    "        # combine the columns into a single row\n",
    "        row_data = []\n",
    "        row_data_extend = row_data.extend\n",
    "        for i in range(len(group)):\n",
    "            for col in group.columns:\n",
    "                if col != 'session_id':\n",
    "                    if pd.notnull(group[col].iloc[i]):\n",
    "                        row_data_extend([group[col].iloc[i]])\n",
    "                    else:\n",
    "                        row_data_extend([np.nan])\n",
    "        # add None values to the row if necessary to make it the same length as the columns\n",
    "        if len(row_data) < len(cols) - 1:\n",
    "            row_data_extend([np.nan] * (len(cols) - len(row_data) - 1))\n",
    "        # add the session_id to the beginning of the row\n",
    "        row_data = [group['session_id'].iloc[0]] + row_data\n",
    "        return pd.Series(row_data, index=cols)\n",
    "\n",
    "    # apply the function to each group and concatenate the results\n",
    "    new_df = pd.concat([flatten_group(group) for _, group in df.groupby('session_id')], axis=1).T\n",
    "\n",
    "    # fill NaN values with np.nan\n",
    "    new_df.fillna(np.nan, inplace=True)\n",
    "\n",
    "    # convert the columns back to their original datatypes\n",
    "    col_dtype_map = {col: df[col].dtype for col in df.columns if col != 'session_id'}\n",
    "    new_df[list(col_dtype_map.keys())] = new_df[list(col_dtype_map.keys())].astype(col_dtype_map, errors='ignore')\n",
    "\n",
    "    # sort the columns\n",
    "    new_df = new_df[cols]\n",
    "\n",
    "    # remove columns that contain only NaN values\n",
    "    new_df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    # remove specified columns from exclude list\n",
    "    for col in exclude:\n",
    "        if col in new_df.columns:\n",
    "            new_df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "    return new_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'session_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mn:\\Python\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mn:\\Python\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mn:\\Python\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'session_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mn:\\MASTER_DS\\Code\\Kaggle_competition\\Kaggle-seminar\\student-performance\\notebooks\\feature_engineer_playground_steve.ipynb Cell 25\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/feature_engineer_playground_steve.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ex \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmusic\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mhq\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mfullscreen\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/feature_engineer_playground_steve.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_0_4 \u001b[39m=\u001b[39m flatten_df_one_at_a_time(df_0_4,exclude\u001b[39m=\u001b[39m ex)\n",
      "\u001b[1;32mn:\\MASTER_DS\\Code\\Kaggle_competition\\Kaggle-seminar\\student-performance\\notebooks\\feature_engineer_playground_steve.ipynb Cell 25\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/feature_engineer_playground_steve.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         df\u001b[39m.\u001b[39mdrop(col, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/feature_engineer_playground_steve.ipynb#X33sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# get the unique session_ids and columns\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/feature_engineer_playground_steve.ipynb#X33sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m session_ids \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39msession_id\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39munique()\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/feature_engineer_playground_steve.ipynb#X33sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m cols \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39msession_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m [\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcol\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(session_ids)) \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39mcolumns \u001b[39mif\u001b[39;00m col \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msession_id\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/feature_engineer_playground_steve.ipynb#X33sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# create a new dataframe to hold the flattened data\u001b[39;00m\n",
      "File \u001b[1;32mn:\\Python\\lib\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3806\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mn:\\Python\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'session_id'"
     ]
    }
   ],
   "source": [
    "ex = [\"level\", \"music\",\"hq\",\"fullscreen\"]\n",
    "df_0_4 = flatten_df_one_at_a_time(df_0_4,exclude= ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session_id                   int64\n",
      "level                        uint8\n",
      "level_group               category\n",
      "event_name                   uint8\n",
      "name                         uint8\n",
      "fqid                         uint8\n",
      "room_fqid                    uint8\n",
      "text_fqid                    uint8\n",
      "fullscreen                   uint8\n",
      "hq                           uint8\n",
      "music                        uint8\n",
      "hover_duration_mean        float32\n",
      "difference_clicks_mean     float32\n",
      "elapsed_time_std           float32\n",
      "page_std                   float32\n",
      "room_coor_x_std            float32\n",
      "room_coor_y_std            float32\n",
      "screen_coor_x_std          float32\n",
      "screen_coor_y_std          float32\n",
      "hover_duration_std         float32\n",
      "difference_clicks_std      float32\n",
      "index_sum_of_actions       float32\n",
      "difference_clicks_max      float32\n",
      "elapsed_time_max           float32\n",
      "clicks_per_second          float32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "dtypes = {\n",
    "    'level': np.uint8,\n",
    "    \"level_group\": \"category\",\n",
    "    'event_name': np.uint8,\n",
    "    'name': np.uint8,\n",
    "    'fqid': np.uint8,\n",
    "    'room_fqid': np.uint8,           \n",
    "    \"text_fqid\": np.uint8,\n",
    "    'fullscreen': np.uint8,\n",
    "    'hq': np.uint8,\n",
    "    'music': np.uint8,\n",
    "    'hover_duration_mean': np.float32,\n",
    "    'difference_clicks_mean': np.float32,\n",
    "    'elapsed_time_std': np.float32,\n",
    "    'page_std': np.float32,\n",
    "    'room_coor_x_std': np.float32,\n",
    "    'room_coor_y_std': np.float32,\n",
    "    'screen_coor_x_std': np.float32,\n",
    "    'screen_coor_y_std': np.float32,\n",
    "    'hover_duration_std': np.float32,\n",
    "    'difference_clicks_std': np.float32,\n",
    "    'index_sum_of_actions': np.float32,\n",
    "    'difference_clicks_max': np.float32,\n",
    "    'elapsed_time_max': np.float32,\n",
    "    'clicks_per_second': np.float32}\n",
    "\n",
    "dataset_df = pd.read_csv(\"data/processed/dataset_df_level.csv\", dtype=dtypes)\n",
    "groups = dataset_df.groupby('level_group')\n",
    "\n",
    "# Create a dictionary to store the resulting dataframes\n",
    "result = {}\n",
    "\n",
    "# Loop over each group\n",
    "for name, group in groups:\n",
    "    # Add the group to the result dictionary\n",
    "    result[name] = group\n",
    "\n",
    "# Access the resulting dataframes using their keys\n",
    "df_0_4 = result['0-4']\n",
    "df_5_12 = result['5-12']\n",
    "df_13_22 = result['13-22']\n",
    "\n",
    "#df_5_12.to_csv('data/processed/df_5_12_flattened.csv')\n",
    "#df_0_4.to_csv('data/processed/df_0_4_flattened.csv')\n",
    "#df_13_22.to_csv('data/processed/df_13_22_flattened.csv')\n",
    "print(df_0_4.dtypes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
