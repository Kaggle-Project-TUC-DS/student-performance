{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports for the Data Preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory:  c:\\Users\\Stephan\\Documents\\Python\\Python-Stat\\Neuer\n"
     ]
    }
   ],
   "source": [
    "# get working directory and remove last folder\n",
    "# TODO: make this more robust\n",
    "wd = os.path.dirname(os.getcwd())\n",
    "os.chdir(wd)\n",
    "print('Working Directory: ', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/raw/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 23\u001b[0m\n\u001b[0;32m      1\u001b[0m dtypes\u001b[39m=\u001b[39m{\n\u001b[0;32m      2\u001b[0m     \u001b[39m'\u001b[39m\u001b[39melapsed_time\u001b[39m\u001b[39m'\u001b[39m:np\u001b[39m.\u001b[39mint32,\n\u001b[0;32m      3\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mevent_name\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmusic\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     19\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlevel_group\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[0;32m     22\u001b[0m \u001b[39m# os.chdir('N:\\MASTER_DS\\Code\\Kaggle_competition\\Kaggle-seminar\\student-performance')\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m dataset_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mdata/raw/train.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mdtypes, nrows\u001b[39m=\u001b[39;49m \u001b[39m20000\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/raw/train.csv'"
     ]
    }
   ],
   "source": [
    "dtypes={\n",
    "    'elapsed_time':np.int32,\n",
    "    'event_name':'category',\n",
    "    'name':'category',\n",
    "    'level':np.uint8,\n",
    "    'room_coor_x':np.float32,\n",
    "    'index':np.int32,\n",
    "    'room_coor_y':np.float32,\n",
    "    'screen_coor_x':np.float32,\n",
    "    'screen_coor_y':np.float32,\n",
    "    'hover_duration':np.float32,\n",
    "    'text':'category',\n",
    "    'fqid':'category',\n",
    "    'room_fqid':'category',\n",
    "    'text_fqid':'category',\n",
    "    'fullscreen':'category',\n",
    "    'hq':'category',\n",
    "    'music':'category',\n",
    "    'level_group':'category'}\n",
    "\n",
    "\n",
    "# os.chdir('N:\\MASTER_DS\\Code\\Kaggle_competition\\Kaggle-seminar\\student-performance')\n",
    "dataset_df = pd.read_csv('data/raw/train.csv', dtype=dtypes, nrows= 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing function to add variables and rescale the time \n",
    "def adding_new_variables_rescaling(dataset_df):\n",
    "    dataset_df = dataset_df.sort_values(['session_id','elapsed_time'])\n",
    "    dataset_df['elapsed_time'] = dataset_df['elapsed_time']/1000\n",
    "    group = dataset_df.groupby(['session_id','level'])['elapsed_time'].diff()\n",
    "    group = group.fillna(value= 0)\n",
    "    dataset_df= dataset_df.assign(difference_clicks = group)\n",
    "\n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing to combine variables on level stage\n",
    "#Function to clean the sequential data for the training of the model\n",
    "\n",
    "#For that Function to work we need to specify the variables in Categorical and Numerical & Counting\n",
    "\n",
    "CATEGORICAL = ['event_name', 'name', 'fqid', 'room_fqid', 'text_fqid', 'fullscreen', 'hq', 'music']\n",
    "NUMERICALmean = ['hover_duration','difference_clicks']\n",
    "NUMERICALstd = ['elapsed_time','page','room_coor_x', 'room_coor_y', 'screen_coor_x', 'screen_coor_y', 'hover_duration', 'difference_clicks']\n",
    "COUNTING = ['index']\n",
    "MAXIMUM = ['difference_clicks', 'elapsed_time']\n",
    "\n",
    "\n",
    "\n",
    "def feature_engineer_steve(dataset_df):\n",
    "    dfs = []\n",
    "    tmp = dataset_df.groupby(['session_id','level'])[\"level_group\"].first()\n",
    "    tmp.name = tmp.name \n",
    "    dfs.append(tmp)\n",
    "    for c in CATEGORICAL:\n",
    "        if c not in ['fullscreen', 'hq', 'music']:\n",
    "            tmp = dataset_df.groupby(['session_id','level'])[c].agg('nunique')\n",
    "        else:\n",
    "            tmp = dataset_df.groupby(['session_id','level'])[c].first().astype(int).fillna(0)\n",
    "        dfs.append(tmp)\n",
    "    for c in NUMERICALmean:\n",
    "        tmp = dataset_df.groupby(['session_id','level'])[c].agg('mean')\n",
    "        tmp.name = tmp.name + '_mean'\n",
    "        dfs.append(tmp)\n",
    "    for c in NUMERICALstd:\n",
    "        tmp = dataset_df.groupby(['session_id','level'])[c].agg('std')\n",
    "        tmp.name = tmp.name + '_std'\n",
    "        dfs.append(tmp)    \n",
    "    for c in COUNTING:\n",
    "        tmp = 1+ dataset_df.groupby(['session_id','level'])[c].agg('max')- dataset_df.groupby(['session_id','level'])[c].agg('min') \n",
    "        tmp.name = tmp.name + '_sum_of_actions'\n",
    "        dfs.append(tmp)\n",
    "    for c in MAXIMUM:\n",
    "        tmp = dataset_df.groupby(['session_id','level'])[c].agg('max')- dataset_df.groupby(['session_id','level'])[c].agg('min') \n",
    "        tmp.name = tmp.name + '_max'\n",
    "        dfs.append(tmp)\n",
    "    \n",
    "    dataset_df = pd.concat(dfs,axis=1)\n",
    "    dataset_df = dataset_df.fillna(-1)\n",
    "    dataset_df = dataset_df.reset_index()\n",
    "    dataset_df = dataset_df.set_index('session_id') \n",
    "    \n",
    "# add Clicks per second afterwards cause we need the time for each level first\n",
    "    dataset_df['clicks_per_second'] = dataset_df['index_sum_of_actions']/ dataset_df['elapsed_time_max']\n",
    " \n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "\n",
    "def clear_memory(keep_list=None):\n",
    "    if keep_list is None:\n",
    "        keep_list = []\n",
    "    for name in dir():\n",
    "        if not name.startswith('_') and name not in keep_list:\n",
    "            value = globals()[name]\n",
    "            if isinstance(value, pd.DataFrame):\n",
    "                del globals()[name]\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              session_id  index  elapsed_time          event_name       name  \\\n",
      "0      20090312431273200      0         0.000      cutscene_click      basic   \n",
      "2      20090312431273200      2         0.831        person_click      basic   \n",
      "3      20090312431273200      3         1.147        person_click      basic   \n",
      "1      20090312431273200      1         1.323        person_click      basic   \n",
      "4      20090312431273200      4         1.863        person_click      basic   \n",
      "...                  ...    ...           ...                 ...        ...   \n",
      "19995  20090317080721164   1364      2454.048  notification_click      basic   \n",
      "19996  20090317080721164   1365      2455.479        object_click      basic   \n",
      "19997  20090317080721164   1366      2456.860        object_hover  undefined   \n",
      "19998  20090317080721164   1367      2457.726        object_click      basic   \n",
      "19999  20090317080721164   1368      2459.226  notification_click      basic   \n",
      "\n",
      "       level  page  room_coor_x  room_coor_y  screen_coor_x  ...  \\\n",
      "0          0   NaN  -413.991394  -159.314682          380.0  ...   \n",
      "2          0   NaN  -413.991394  -159.314682          380.0  ...   \n",
      "3          0   NaN  -413.991394  -159.314682          380.0  ...   \n",
      "1          0   NaN  -413.991394  -159.314682          380.0  ...   \n",
      "4          0   NaN  -412.991394  -159.314682          381.0  ...   \n",
      "...      ...   ...          ...          ...            ...  ...   \n",
      "19995     21   NaN   406.725525   -73.244423          771.0  ...   \n",
      "19996     21   NaN   406.725525   -73.244423          771.0  ...   \n",
      "19997     21   NaN          NaN          NaN            NaN  ...   \n",
      "19998     21   NaN  -101.274475    15.755580          263.0  ...   \n",
      "19999     21   NaN   423.725525   -83.244423          788.0  ...   \n",
      "\n",
      "       hover_duration                                         text  \\\n",
      "0                 NaN                                    undefined   \n",
      "2                 NaN                       Just talking to Teddy.   \n",
      "3                 NaN                   I gotta run to my meeting!   \n",
      "1                 NaN                Whatcha doing over there, Jo?   \n",
      "4                 NaN                          Can I come, Gramps?   \n",
      "...               ...                                          ...   \n",
      "19995             NaN  I should go to the Capitol and tell Mrs. M!   \n",
      "19996             NaN                                          NaN   \n",
      "19997          7826.0                                          NaN   \n",
      "19998             NaN                                          NaN   \n",
      "19999             NaN                 Look at all those activists!   \n",
      "\n",
      "                            fqid                       room_fqid  \\\n",
      "0                          intro  tunic.historicalsociety.closet   \n",
      "2                         gramps  tunic.historicalsociety.closet   \n",
      "3                         gramps  tunic.historicalsociety.closet   \n",
      "1                         gramps  tunic.historicalsociety.closet   \n",
      "4                         gramps  tunic.historicalsociety.closet   \n",
      "...                          ...                             ...   \n",
      "19995                        NaN  tunic.historicalsociety.stacks   \n",
      "19996   journals_flag.pic_2.next  tunic.historicalsociety.stacks   \n",
      "19997  journals_flag.pic_2.bingo  tunic.historicalsociety.stacks   \n",
      "19998  journals_flag.pic_0.bingo  tunic.historicalsociety.stacks   \n",
      "19999                        NaN  tunic.historicalsociety.stacks   \n",
      "\n",
      "                                               text_fqid fullscreen hq music  \\\n",
      "0                   tunic.historicalsociety.closet.intro          0  0     1   \n",
      "2      tunic.historicalsociety.closet.gramps.intro_0_...          0  0     1   \n",
      "3      tunic.historicalsociety.closet.gramps.intro_0_...          0  0     1   \n",
      "1      tunic.historicalsociety.closet.gramps.intro_0_...          0  0     1   \n",
      "4      tunic.historicalsociety.closet.gramps.intro_0_...          0  0     1   \n",
      "...                                                  ...        ... ..   ...   \n",
      "19995  tunic.historicalsociety.stacks.journals_flag.p...          0  0     1   \n",
      "19996                                                NaN          0  0     1   \n",
      "19997                                                NaN          0  0     1   \n",
      "19998                                                NaN          0  0     1   \n",
      "19999  tunic.historicalsociety.stacks.journals_flag.p...          0  0     1   \n",
      "\n",
      "      level_group difference_clicks  \n",
      "0             0-4             0.000  \n",
      "2             0-4             0.831  \n",
      "3             0-4             0.316  \n",
      "1             0-4             0.176  \n",
      "4             0-4             0.540  \n",
      "...           ...               ...  \n",
      "19995       13-22             0.808  \n",
      "19996       13-22             1.431  \n",
      "19997       13-22             1.381  \n",
      "19998       13-22             0.866  \n",
      "19999       13-22             1.500  \n",
      "\n",
      "[20000 rows x 21 columns]\n",
      "                   level level_group  event_name  name  fqid  room_fqid  \\\n",
      "session_id                                                                \n",
      "20090312431273200      0         0-4           6     3     5          1   \n",
      "20090312431273200      1         0-4           6     3     6          3   \n",
      "20090312431273200      2         0-4           6     3     8          2   \n",
      "20090312431273200      3         0-4           9     3    13          6   \n",
      "20090312431273200      4         0-4           4     2     5          2   \n",
      "...                  ...         ...         ...   ...   ...        ...   \n",
      "20090317080721164     17       13-22           4     4     5          4   \n",
      "20090317080721164     18       13-22          10     4    17          3   \n",
      "20090317080721164     19       13-22           7     4     9          2   \n",
      "20090317080721164     20       13-22           7     3    14          3   \n",
      "20090317080721164     21       13-22           8     4    15          5   \n",
      "\n",
      "                   text_fqid  fullscreen  hq  music  ...  room_coor_x_std  \\\n",
      "session_id                                           ...                    \n",
      "20090312431273200          6           0   0      1  ...       469.524328   \n",
      "20090312431273200          3           0   0      1  ...       358.166662   \n",
      "20090312431273200          5           0   0      1  ...       324.386196   \n",
      "20090312431273200          5           0   0      1  ...       389.874453   \n",
      "20090312431273200          0           0   0      1  ...       422.014525   \n",
      "...                      ...         ...  ..    ...  ...              ...   \n",
      "20090317080721164          2           0   0      1  ...       605.727392   \n",
      "20090317080721164         14           0   0      1  ...       539.819725   \n",
      "20090317080721164          5           0   0      1  ...       699.746370   \n",
      "20090317080721164          3           0   0      1  ...       277.277660   \n",
      "20090317080721164          4           0   0      1  ...       332.716483   \n",
      "\n",
      "                   room_coor_y_std  screen_coor_x_std  screen_coor_y_std  \\\n",
      "session_id                                                                 \n",
      "20090312431273200        84.862476         211.047234          87.147142   \n",
      "20090312431273200       116.538196         188.452544          89.020102   \n",
      "20090312431273200       158.503091         178.524529         106.028716   \n",
      "20090312431273200       117.002236         204.998931         106.129303   \n",
      "20090312431273200        41.143422         181.938451          39.566540   \n",
      "...                            ...                ...                ...   \n",
      "20090317080721164       190.298909         254.499702         126.715013   \n",
      "20090317080721164       278.060536         188.042548          94.995875   \n",
      "20090317080721164       334.139127         285.106399         152.887989   \n",
      "20090317080721164       121.906446         190.157612         114.305189   \n",
      "20090317080721164       126.627917         222.888578         112.787849   \n",
      "\n",
      "                   hover_duration_std  difference_clicks_std  \\\n",
      "session_id                                                     \n",
      "20090312431273200           -1.000000               0.473709   \n",
      "20090312431273200           -1.000000               1.068973   \n",
      "20090312431273200         2509.521966               0.520979   \n",
      "20090312431273200         3595.354271               0.886427   \n",
      "20090312431273200          164.755880               8.380692   \n",
      "...                               ...                    ...   \n",
      "20090317080721164           -1.000000               0.983913   \n",
      "20090317080721164         4256.798661               1.662691   \n",
      "20090317080721164          117.126712               1.301016   \n",
      "20090317080721164         3196.247205               1.395779   \n",
      "20090317080721164         2964.786606               1.445493   \n",
      "\n",
      "                   index_sum_of_actions  difference_clicks_max  \\\n",
      "session_id                                                       \n",
      "20090312431273200                    28                  1.784   \n",
      "20090312431273200                    32                  4.864   \n",
      "20090312431273200                    39                  2.215   \n",
      "20090312431273200                    53                  4.540   \n",
      "20090312431273200                    13                 30.837   \n",
      "...                                 ...                    ...   \n",
      "20090317080721164                    29                  4.382   \n",
      "20090317080721164                   135                 14.273   \n",
      "20090317080721164                    67                  6.045   \n",
      "20090317080721164                    49                  8.418   \n",
      "20090317080721164                    54                  8.560   \n",
      "\n",
      "                   elapsed_time_max  clicks_per_second  \n",
      "session_id                                              \n",
      "20090312431273200            25.766           1.086703  \n",
      "20090312431273200            37.346           0.856852  \n",
      "20090312431273200            34.253           1.138586  \n",
      "20090312431273200            54.712           0.968709  \n",
      "20090312431273200            38.621           0.336604  \n",
      "...                             ...                ...  \n",
      "20090317080721164            40.770           0.711307  \n",
      "20090317080721164           230.340           0.586090  \n",
      "20090317080721164            98.990           0.676836  \n",
      "20090317080721164            67.548           0.725410  \n",
      "20090317080721164            91.930           0.587403  \n",
      "\n",
      "[412 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "#return the preprocessed data\n",
    "##test data preprocessing with Subset of 5 million rows\n",
    "\n",
    "dataset_df_added = adding_new_variables_rescaling(dataset_df)\n",
    "dataset_df_level = feature_engineer_steve(dataset_df_added)\n",
    "print(dataset_df_added)\n",
    "print(dataset_df_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "#dataset_df_added.to_csv('data/processed/data_df_added.csv')\n",
    "#dataset_df_level.to_csv('data/processed/data_df_level.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level                        uint8\n",
      "level_group               category\n",
      "event_name                   uint8\n",
      "name                         uint8\n",
      "fqid                         uint8\n",
      "room_fqid                    uint8\n",
      "text_fqid                    uint8\n",
      "fullscreen                   uint8\n",
      "hq                           uint8\n",
      "music                        uint8\n",
      "hover_duration_mean          int32\n",
      "difference_clicks_mean       int32\n",
      "elapsed_time_std             int32\n",
      "page_std                     int32\n",
      "room_coor_x_std              int32\n",
      "room_coor_y_std              int32\n",
      "screen_coor_x_std            int32\n",
      "screen_coor_y_std            int32\n",
      "hover_duration_std           int32\n",
      "difference_clicks_std        int32\n",
      "index_sum_of_actions         int32\n",
      "difference_clicks_max        int32\n",
      "elapsed_time_max             int32\n",
      "clicks_per_second            int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#now as a function\n",
    "#split the dataframe into three different ones depending on the level group\n",
    "dtypes = {\n",
    "    'level': np.uint8,\n",
    "    \"level_group\": \"category\",\n",
    "    'event_name': np.uint8,\n",
    "    'name': np.uint8,\n",
    "    'fqid': np.uint8,\n",
    "    'room_fqid': np.uint8,           \n",
    "    \"text_fqid\": np.uint8,\n",
    "    'fullscreen': np.uint8,\n",
    "    'hq': np.uint8,\n",
    "    'music': np.uint8,\n",
    "    'hover_duration_mean': np.int32,\n",
    "    'difference_clicks_mean': np.int32,\n",
    "    'elapsed_time_std': np.int32,\n",
    "    'page_std': np.int32,\n",
    "    'room_coor_x_std': np.int32,\n",
    "    'room_coor_y_std': np.int32,\n",
    "    'screen_coor_x_std': np.int32,\n",
    "    'screen_coor_y_std': np.int32,\n",
    "    'hover_duration_std': np.int32,\n",
    "    'difference_clicks_std': np.int32,\n",
    "    'index_sum_of_actions': np.int32,\n",
    "    'difference_clicks_max': np.int32,\n",
    "    'elapsed_time_max': np.int32,\n",
    "    'clicks_per_second': np.int32}\n",
    "groups = dataset_df_level.groupby('level_group')\n",
    "\n",
    "# Create a dictionary to store the resulting dataframes\n",
    "result = {}\n",
    "\n",
    "# Loop over each group\n",
    "for name, group in groups:\n",
    "    # Add the group to the result dictionary\n",
    "    result[name] = group.astype(dtypes)\n",
    "\n",
    "# Access the resulting dataframes using their keys\n",
    "df_0_4 = result['0-4']\n",
    "df_5_12 = result['5-12']\n",
    "df_13_22 = result['13-22']\n",
    "print(df_0_4.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as a function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def split_data_by_level_group(df, dtypes):\n",
    "    # Define the data types for the columns\n",
    "    \n",
    "    # Split the DataFrame into three based on the 'level_group' column\n",
    "    df_0_4 = df[df['level_group'] == '0-4'].astype(dtypes)\n",
    "    df_5_12 = df[df['level_group'] == '5-12'].astype(dtypes)\n",
    "    df_13_22 = df[df['level_group'] == '13-22'].astype(dtypes)\n",
    "    \n",
    "    # Return the three resulting DataFrames\n",
    "    return df_0_4, df_5_12, df_13_22\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'level': np.uint8,\n",
    "    \"level_group\": \"category\",\n",
    "    'event_name': np.uint8,\n",
    "    'name': np.uint8,\n",
    "    'fqid': np.uint8,\n",
    "    'room_fqid': np.uint8,           \n",
    "    \"text_fqid\": np.uint8,\n",
    "    'fullscreen': np.uint8,\n",
    "    'hq': np.uint8,\n",
    "    'music': np.uint8,\n",
    "    'hover_duration_mean': np.int32,\n",
    "    'difference_clicks_mean': np.int32,\n",
    "    'elapsed_time_std': np.int32,\n",
    "    'page_std': np.int32,\n",
    "    'room_coor_x_std': np.int32,\n",
    "    'room_coor_y_std': np.int32,\n",
    "    'screen_coor_x_std': np.int32,\n",
    "    'screen_coor_y_std': np.int32,\n",
    "    'hover_duration_std': np.int32,\n",
    "    'difference_clicks_std': np.int32,\n",
    "    'index_sum_of_actions': np.int32,\n",
    "    'difference_clicks_max': np.int32,\n",
    "    'elapsed_time_max': np.int32,\n",
    "    'clicks_per_second': np.int32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def flatten_df(df):\n",
    "    # create a new index col\n",
    "    df = df.reset_index()\n",
    "    df['_index'] = df.index + 1\n",
    "    df = df.set_index('_index')\n",
    "    df = df.drop([\"level_group\"], axis= 1)\n",
    "\n",
    "    # get the unique session_ids and columns\n",
    "    session_ids = df['session_id'].unique()\n",
    "    cols = ['session_id'] + [f'{col}_{i+1}' for i in range(len(session_ids)) for col in df.columns if col != 'session_id']\n",
    "\n",
    "    # create a new dataframe to hold the flattened data\n",
    "    new_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    # define a function to apply to each group\n",
    "    def flatten_group(group):\n",
    "        # combine the columns into a single row\n",
    "        row_data = [group[col].iloc[i] if pd.notnull(group[col].iloc[i]) else np.nan for i in range(len(group)) for col in group.columns if col != 'session_id'] \n",
    "        # add None values to the row if necessary to make it the same length as the columns\n",
    "        if len(row_data) < len(cols) - 1:\n",
    "            row_data += [np.nan] * (len(cols) - len(row_data) - 1)\n",
    "        # add the session_id to the beginning of the row\n",
    "        row_data = [group['session_id'].iloc[0]] + row_data\n",
    "        return pd.Series(row_data, index=cols)\n",
    "\n",
    "    # apply the function to each group and concatenate the results\n",
    "    new_df = pd.concat([flatten_group(group) for _, group in df.groupby('session_id')], axis=1).T\n",
    "\n",
    "    # fill NaN values with np.nan\n",
    "    new_df = new_df.fillna(np.nan)\n",
    "\n",
    "    # convert the columns back to their original datatypes\n",
    "    for col in df.columns:\n",
    "        if col != 'session_id':\n",
    "            new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]] = new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]].astype(df[col].dtype, errors='ignore')\n",
    "\n",
    "    # sort the columns\n",
    "    new_df = new_df[cols]\n",
    "\n",
    "    # remove columns that contain only NaN values\n",
    "    new_df = new_df.dropna(axis=1, how='all')\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#outdated\n",
    "def flatten_df(df):\n",
    "    # create a new index col\n",
    "    df = df.reset_index()\n",
    "    df['_index'] = df.index + 1\n",
    "    df = df.set_index('_index')\n",
    "    df = df.drop([\"level_group\"], axis= 1)\n",
    "\n",
    "    # get the unique session_ids and columns\n",
    "    session_ids = df['session_id'].unique()\n",
    "    cols = ['session_id'] + [f'{col}_{i+1}' for i in range(len(session_ids)) for col in df.columns if col != 'session_id']\n",
    "\n",
    "    # create a new dataframe to hold the flattened data\n",
    "    new_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    # define a function to apply to each group\n",
    "    def flatten_group(group):\n",
    "        # combine the columns into a single row\n",
    "        row_data = [group[col].iloc[i] if pd.notnull(group[col].iloc[i]) else np.nan for i in range(len(group)) for col in group.columns if col != 'session_id'] \n",
    "        # add None values to the row if necessary to make it the same length as the columns\n",
    "        if len(row_data) < len(cols) - 1:\n",
    "            row_data += [np.nan] * (len(cols) - len(row_data) - 1)\n",
    "        # add the session_id to the beginning of the row\n",
    "        row_data = [group['session_id'].iloc[0]] + row_data\n",
    "        return pd.Series(row_data, index=cols)\n",
    "\n",
    "    # apply the function to each group and concatenate the results\n",
    "    new_df = pd.concat([flatten_group(group) for _, group in df.groupby('session_id')], axis=1).T\n",
    "\n",
    "    # fill NaN values with np.nan\n",
    "    new_df = new_df.fillna(np.nan)\n",
    "\n",
    "    # convert the columns back to their original datatypes\n",
    "    for col in df.columns:\n",
    "        if col != 'session_id':\n",
    "            new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]] = new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]].astype(df[col].dtype, errors='ignore')\n",
    "\n",
    "    # sort the columns\n",
    "    new_df = new_df[cols]\n",
    "\n",
    "    # remove columns that contain only NaN values\n",
    "    new_df = new_df.dropna(axis=1, how='all')\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#outdated\n",
    "def flatten_df(df, exclude=[]):\n",
    "    # create a new index col\n",
    "    df = df.reset_index()\n",
    "    df['_index'] = df.index + 1\n",
    "    df = df.set_index('_index')\n",
    "    df = df.drop([\"level_group\"], axis= 1)\n",
    "\n",
    "    # get the unique session_ids and columns\n",
    "    session_ids = df['session_id'].unique()\n",
    "    cols = ['session_id'] + [f'{col}_{i+1}' for i in range(len(session_ids)) for col in df.columns if col != 'session_id']\n",
    "\n",
    "    # create a new dataframe to hold the flattened data\n",
    "    new_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    # define a function to apply to each group\n",
    "    def flatten_group(group):\n",
    "        # combine the columns into a single row\n",
    "        row_data = [group[col].iloc[i] if pd.notnull(group[col].iloc[i]) else np.nan for i in range(len(group)) for col in group.columns if col != 'session_id'] \n",
    "        # add None values to the row if necessary to make it the same length as the columns\n",
    "        if len(row_data) < len(cols) - 1:\n",
    "            row_data += [np.nan] * (len(cols) - len(row_data) - 1)\n",
    "        # add the session_id to the beginning of the row\n",
    "        row_data = [group['session_id'].iloc[0]] + row_data\n",
    "        return pd.Series(row_data, index=cols)\n",
    "\n",
    "    # apply the function to each group and concatenate the results\n",
    "    new_df = pd.concat([flatten_group(group) for _, group in df.groupby('session_id')], axis=1).T\n",
    "\n",
    "    # fill NaN values with np.nan\n",
    "    new_df = new_df.fillna(np.nan)\n",
    "\n",
    "    # convert the columns back to their original datatypes\n",
    "    for col in df.columns:\n",
    "        if col != 'session_id':\n",
    "            new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]] = new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]].astype(df[col].dtype, errors='ignore')\n",
    "\n",
    "    # sort the columns\n",
    "    new_df = new_df[cols]\n",
    "\n",
    "    # remove columns that contain only NaN values\n",
    "    new_df = new_df.dropna(axis=1, how='all')\n",
    "\n",
    "    # remove specified columns from exclude list\n",
    "    for col_name in exclude:\n",
    "        new_df = new_df[[col for col in new_df.columns if not (col.startswith(col_name + '_') and col != col_name + '_1')]]\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#outdated\n",
    "def flatten_df(dataset_df_level, exclude=[]):\n",
    "    #split the dataframe into three different ones depending on the level group\n",
    "    groups = dataset_df_level.groupby('level_group')\n",
    "\n",
    "    # Create a dictionary to store the resulting dataframes\n",
    "    result = {}\n",
    "\n",
    "    # Loop over each group\n",
    "    for name, group in groups:\n",
    "        # Add the group to the result dictionary\n",
    "        result[name] = group\n",
    "\n",
    "    # Access the resulting dataframes using their keys\n",
    "    df_0_4 = result['0-4']\n",
    "    df_5_12 = result['5-12']\n",
    "    df_13_22 = result['13-22']\n",
    "\n",
    "    dfs = [df_0_4, df_5_12, df_13_22]\n",
    "    flattened_dfs = []\n",
    "\n",
    "    for df in dfs:\n",
    "        # create a new index col\n",
    "        df = df.reset_index()\n",
    "        df['_index'] = df.index + 1\n",
    "        df = df.set_index('_index')\n",
    "        df = df.drop([\"level_group\"], axis= 1)\n",
    "\n",
    "        # get the unique session_ids and columns\n",
    "        session_ids = df['session_id'].unique()\n",
    "        cols = ['session_id'] + [f'{col}_{i+1}' for i in range(len(session_ids)) for col in df.columns if col != 'session_id']\n",
    "\n",
    "        # create a new dataframe to hold the flattened data\n",
    "        new_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "        # define a function to apply to each group\n",
    "        def flatten_group(group):\n",
    "            # combine the columns into a single row\n",
    "            row_data = [group[col].iloc[i] if pd.notnull(group[col].iloc[i]) else np.nan for i in range(len(group)) for col in group.columns if col != 'session_id'] \n",
    "            # add None values to the row if necessary to make it the same length as the columns\n",
    "            if len(row_data) < len(cols) - 1:\n",
    "                row_data += [np.nan] * (len(cols) - len(row_data) - 1)\n",
    "            # add the session_id to the beginning of the row\n",
    "            row_data = [group['session_id'].iloc[0]] + row_data\n",
    "            return pd.Series(row_data, index=cols)\n",
    "\n",
    "        # apply the function to each group and concatenate the results\n",
    "        new_df = pd.concat([flatten_group(group) for _, group in df.groupby('session_id')], axis=1).T\n",
    "\n",
    "        # fill NaN values with np.nan\n",
    "        new_df = new_df.fillna(np.nan)\n",
    "\n",
    "        # convert the columns back to their original datatypes\n",
    "        for col in df.columns:\n",
    "            if col != 'session_id':\n",
    "                new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]] = new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]].astype(df[col].dtype, errors='ignore')\n",
    "\n",
    "        # sort the columns\n",
    "        new_df = new_df[cols]\n",
    "\n",
    "        # remove columns that contain only NaN values\n",
    "        new_df = new_df.dropna(axis=1, how='all')\n",
    "\n",
    "        # remove specified columns from exclude list\n",
    "        for col_name in exclude:\n",
    "            new_df = new_df[[col for col in new_df.columns if not (col.startswith(col_name + '_') and col != col_name + '_1')]]\n",
    "\n",
    "        flattened_dfs.append(new_df)\n",
    "\n",
    "    return flattened_dfs[0], flattened_dfs[1], flattened_dfs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outdated\n",
    "def flatten_df_one_at_a_time(df, exclude=[]):\n",
    "    # create a new index col\n",
    "    df = df.reset_index()\n",
    "    df['_index'] = df.index + 1\n",
    "    df = df.set_index('_index')\n",
    "    df = df.drop([\"level_group\"], axis=1)\n",
    "\n",
    "    # get the unique session_ids and columns\n",
    "    session_ids = df['session_id'].unique()\n",
    "    cols = ['session_id'] + [f'{col}_{i+1}' for i in range(len(session_ids)) for col in df.columns if col != 'session_id']\n",
    "\n",
    "    # create a new dataframe to hold the flattened data\n",
    "    new_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    # define a function to apply to each group\n",
    "    def flatten_group(group):\n",
    "        # combine the columns into a single row\n",
    "        row_data = [group[col].iloc[i] if pd.notnull(group[col].iloc[i]) else np.nan for i in range(len(group)) for col in group.columns if col != 'session_id']\n",
    "        # add None values to the row if necessary to make it the same length as the columns\n",
    "        if len(row_data) < len(cols) - 1:\n",
    "            row_data += [np.nan] * (len(cols) - len(row_data) - 1)\n",
    "        # add the session_id to the beginning of the row\n",
    "        row_data = [group['session_id'].iloc[0]] + row_data\n",
    "        return pd.Series(row_data, index=cols)\n",
    "\n",
    "    # apply the function to each group and concatenate the results\n",
    "    new_df = pd.concat([flatten_group(group) for _, group in df.groupby('session_id')], axis=1).T\n",
    "\n",
    "    # fill NaN values with np.nan\n",
    "    new_df = new_df.fillna(np.nan)\n",
    "\n",
    "    # convert the columns back to their original datatypes\n",
    "    for col in df.columns:\n",
    "        if col != 'session_id':\n",
    "            new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]] = new_df[[f'{col}_{i+1}' for i in range(len(session_ids))]].astype(df[col].dtype, errors='ignore')\n",
    "\n",
    "    # sort the columns\n",
    "    new_df = new_df[cols]\n",
    "\n",
    "    # remove columns that contain only NaN values\n",
    "    new_df = new_df.dropna(axis=1, how='all')\n",
    "\n",
    "    # remove specified columns from exclude list\n",
    "    for col_name in exclude:\n",
    "        new_df = new_df[[col for col in new_df.columns if not (col.startswith(col_name + '_') and col != col_name + '_1')]]\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/processed/dataset_df_level.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 27\u001b[0m\n\u001b[0;32m      1\u001b[0m dtypes \u001b[39m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m'\u001b[39m: np\u001b[39m.\u001b[39muint8,\n\u001b[0;32m      3\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlevel_group\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[39m'\u001b[39m\u001b[39melapsed_time_max\u001b[39m\u001b[39m'\u001b[39m: np\u001b[39m.\u001b[39mfloat32,\n\u001b[0;32m     25\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mclicks_per_second\u001b[39m\u001b[39m'\u001b[39m: np\u001b[39m.\u001b[39mfloat32}\n\u001b[1;32m---> 27\u001b[0m dataset_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mdata/processed/dataset_df_level.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mdtypes, nrows\u001b[39m=\u001b[39;49m \u001b[39m50000\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/processed/dataset_df_level.csv'"
     ]
    }
   ],
   "source": [
    "dtypes = {\n",
    "    'level': np.uint8,\n",
    "    \"level_group\": \"category\",\n",
    "    'event_name': np.uint8,\n",
    "    'name': np.uint8,\n",
    "    'fqid': np.uint8,\n",
    "    'room_fqid': np.uint8,           \n",
    "    \"text_fqid\": np.uint8,\n",
    "    'fullscreen': np.uint8,\n",
    "    'hq': np.uint8,\n",
    "    'music': np.uint8,\n",
    "    'hover_duration_mean': np.float32,\n",
    "    'difference_clicks_mean': np.float32,\n",
    "    'elapsed_time_std': np.float32,\n",
    "    'page_std': np.float32,\n",
    "    'room_coor_x_std': np.float32,\n",
    "    'room_coor_y_std': np.float32,\n",
    "    'screen_coor_x_std': np.float32,\n",
    "    'screen_coor_y_std': np.float32,\n",
    "    'hover_duration_std': np.float32,\n",
    "    'difference_clicks_std': np.float32,\n",
    "    'index_sum_of_actions': np.float32,\n",
    "    'difference_clicks_max': np.float32,\n",
    "    'elapsed_time_max': np.float32,\n",
    "    'clicks_per_second': np.float32}\n",
    "\n",
    "dataset_df = pd.read_csv(\"data/processed/dataset_df_level.csv\", dtype=dtypes, nrows= 50000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1, d2, df3 = split_data_by_level_group(dataset_df, dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_rows(df, n_flatten=5, only_one=None, drop=None):\n",
    "    \"\"\"\n",
    "    Combines every n_flatten rows of a Pandas DataFrame into a new DataFrame, with each row containing the combined values from the n_flatten rows.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to combine.\n",
    "        n_flatten (int): The number of rows to be combined into a single row.\n",
    "        only_one (list): A list of column names to keep only the first occurrence of in the output DataFrame.\n",
    "        drop (list): A list of column names to drop from the input DataFrame before performing the calculation.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A new DataFrame containing one row for every n_flatten rows of the input DataFrame, with each row containing the combined values from the n_flatten rows.\n",
    "    \"\"\"\n",
    "    # Create a copy of the input DataFrame to modify\n",
    "    df = df.copy()\n",
    "\n",
    "    # Drop specified columns from input DataFrame\n",
    "    if drop:\n",
    "        df = df.drop(columns=drop)\n",
    "\n",
    "    # Check if all session_id's occur the same amount of times\n",
    "    session_counts = df['session_id'].value_counts()\n",
    "    if len(set(session_counts.values)) > 1:\n",
    "        raise ValueError(\"Missing level: All session_id's should occur the same amount of times.\")\n",
    "\n",
    "    # Determine the number of rows and columns in the input DataFrame\n",
    "    num_rows, num_cols = df.shape\n",
    "\n",
    "    # Determine the number of new rows in the output DataFrame\n",
    "    num_new_rows = num_rows // n_flatten\n",
    "\n",
    "    # Reshape the flattened values into a new array with the desired shape\n",
    "    values = df.values.flatten()\n",
    "    new_values = values.reshape(num_new_rows, n_flatten*num_cols)\n",
    "\n",
    "    # Create a new DataFrame from the reshaped values\n",
    "    new_df = pd.DataFrame(new_values, columns=[f\"{col}_{i}\" for i in range(1, n_flatten+1) for col in df.columns])\n",
    "\n",
    "    # Drop specified columns from output DataFrame\n",
    "    if only_one:\n",
    "        for col in only_one:\n",
    "            keep_col = f\"{col}_1\"\n",
    "            drop_cols = [f\"{col}_{i}\" for i in range(2, n_flatten+1)]\n",
    "            new_df = new_df.drop(columns=drop_cols)\n",
    "\n",
    "    return new_df\n",
    "def generate_rows(df: pd.DataFrame, n_flatten: int, level_g: str):\n",
    "    # Use value_counts() to get the count of each session_id\n",
    "    counts = df['session_id'].value_counts()\n",
    "\n",
    "    # Check if each group has the same number of rows\n",
    "    if (counts % n_flatten).any():\n",
    "        # Get the session_ids that need to be generated\n",
    "        need_generating = counts[counts < n_flatten].index.tolist()\n",
    "        num_generated_rows = 0\n",
    "        generated_sessions = []\n",
    "        generated_rows = []\n",
    "\n",
    "        # Loop through the session_ids that need to be generated\n",
    "        for session_id in need_generating:\n",
    "            # Check if all levels are present in this session\n",
    "            levels_present = set(df.loc[df['session_id'] == session_id, 'level'].unique())\n",
    "            min_level = df['level'].min()\n",
    "            max_level = df['level'].max()\n",
    "            expected_levels = set(range(min_level, max_level + 1))\n",
    "            if levels_present != expected_levels:\n",
    "                # Generate new rows for missing levels\n",
    "                missing_levels = expected_levels - levels_present\n",
    "                new_rows = []\n",
    "                for missing_level in missing_levels:\n",
    "                    new_row = {\"session_id\": session_id, \"level\": missing_level}\n",
    "                    for col in df.columns:\n",
    "                        if col == \"session_id\" or col == \"level\":\n",
    "                            continue\n",
    "                        \n",
    "                        else:\n",
    "                            # Numeric column - set value to average of other values in column with the same level\n",
    "                            other_values = df.loc[(df[\"session_id\"] == session_id) & (df[\"level\"] == missing_level), col]\n",
    "                            if other_values.dtype.kind in 'biufc':\n",
    "                                new_value = other_values.mean()\n",
    "                                if np.isnan(new_value):\n",
    "                                    new_value = df.loc[df[\"level\"] == missing_level, col].mean()\n",
    "                                new_row[col] = new_value\n",
    "                    new_rows.append(new_row)\n",
    "\n",
    "                # Add the new rows to the dataframe\n",
    "                df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "                num_generated_rows += len(new_rows)\n",
    "                generated_sessions.append({\"session_id\": session_id, \"num_rows\": len(new_rows)})\n",
    "                generated_rows.extend(new_rows)\n",
    "\n",
    "        print(f\"Generated {num_generated_rows} rows with indices: {list(range(len(df) - num_generated_rows, len(df)))}\")\n",
    "\n",
    "        # Create output dataframe 2\n",
    "        df2 = pd.DataFrame(generated_sessions)\n",
    "        df2 = df2.set_index(\"session_id\")\n",
    "        print(\"Generated rows per session id:\")\n",
    "        print(df2)\n",
    "\n",
    "        # Create output dataframe 3\n",
    "        df3 = pd.DataFrame(generated_rows)\n",
    "        df3 = df3.reindex(df.columns, axis=1)\n",
    "        print(\"Generated rows:\")\n",
    "        print(df3)\n",
    "    else:\n",
    "        df2 = pd.DataFrame()\n",
    "        df3 = pd.DataFrame()\n",
    "    df[\"level_group\"] = level_g\n",
    "    df3[\"level_group\"] = level_g\n",
    "    return df, df2, df3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df5 , check, check1 \u001b[39m=\u001b[39m generate_rows( df3, n_flatten\u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m, level_g \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m5-12\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[13], line 78\u001b[0m, in \u001b[0;36mgenerate_rows\u001b[1;34m(df, n_flatten, level_g)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m     \u001b[39m# Numeric column - set value to average of other values in column with the same level\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m     other_values \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mloc[(df[\u001b[39m\"\u001b[39m\u001b[39msession_id\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m session_id) \u001b[39m&\u001b[39m (df[\u001b[39m\"\u001b[39;49m\u001b[39mlevel\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m==\u001b[39;49m missing_level), col]\n\u001b[0;32m     79\u001b[0m     \u001b[39mif\u001b[39;00m other_values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mbiufc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     80\u001b[0m         new_value \u001b[39m=\u001b[39m other_values\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\ops\\common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[0;32m     70\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 72\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\arraylike.py:42\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__eq__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__eq__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cmp_method(other, operator\u001b[39m.\u001b[39;49meq)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\series.py:6243\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6240\u001b[0m rvalues \u001b[39m=\u001b[39m extract_array(other, extract_numpy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, extract_range\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   6242\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 6243\u001b[0m     res_values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mcomparison_op(lvalues, rvalues, op)\n\u001b[0;32m   6245\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_result(res_values, name\u001b[39m=\u001b[39mres_name)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\ops\\array_ops.py:290\u001b[0m, in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    287\u001b[0m     res_values \u001b[39m=\u001b[39m comp_method_OBJECT_ARRAY(op, lvalues, rvalues)\n\u001b[0;32m    289\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 290\u001b[0m     res_values \u001b[39m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    292\u001b[0m \u001b[39mreturn\u001b[39;00m res_values\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\ops\\array_ops.py:165\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    162\u001b[0m     func \u001b[39m=\u001b[39m partial(expressions\u001b[39m.\u001b[39mevaluate, op)\n\u001b[0;32m    164\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     result \u001b[39m=\u001b[39m func(left, right)\n\u001b[0;32m    166\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    167\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_cmp \u001b[39mand\u001b[39;00m (is_object_dtype(left\u001b[39m.\u001b[39mdtype) \u001b[39mor\u001b[39;00m is_object_dtype(right)):\n\u001b[0;32m    168\u001b[0m         \u001b[39m# For object dtype, fallback to a masked operation (only operating\u001b[39;00m\n\u001b[0;32m    169\u001b[0m         \u001b[39m#  on the non-missing values)\u001b[39;00m\n\u001b[0;32m    170\u001b[0m         \u001b[39m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[0;32m    171\u001b[0m         \u001b[39m#  incorrectly, see GH#32047\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\computation\\expressions.py:241\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m op_str \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[39mif\u001b[39;00m use_numexpr:\n\u001b[0;32m    240\u001b[0m         \u001b[39m# error: \"None\" not callable\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m         \u001b[39mreturn\u001b[39;00m _evaluate(op, op_str, a, b)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[39mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\computation\\expressions.py:70\u001b[0m, in \u001b[0;36m_evaluate_standard\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m _TEST_MODE:\n\u001b[0;32m     69\u001b[0m     _store_test_result(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> 70\u001b[0m \u001b[39mreturn\u001b[39;00m op(a, b)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df5 , check, check1 = generate_rows( df3, n_flatten= 10, level_g = \"5-12\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
