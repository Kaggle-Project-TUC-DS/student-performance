{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports for the Data Preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import os\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get working directory and remove last folder\n",
    "# TODO: make this more robust\n",
    "wd = os.path.dirname(os.getcwd())\n",
    "os.chdir(wd)\n",
    "print('Working Directory: ', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'level': np.uint8,\n",
    "    \"level_group\": \"category\",\n",
    "    'event_name': np.uint8,\n",
    "    'name': np.uint8,\n",
    "    'fqid': np.uint8,\n",
    "    'room_fqid': np.uint8,           \n",
    "    \"text_fqid\": np.uint8,\n",
    "    'fullscreen': np.uint8,\n",
    "    'hq': np.uint8,\n",
    "    'music': np.uint8,\n",
    "    'hover_duration_mean': np.float32,\n",
    "    'difference_clicks_mean': np.float32,\n",
    "    'elapsed_time_std': np.float32,\n",
    "    'page_std': np.float32,\n",
    "    'room_coor_x_std': np.float32,\n",
    "    'room_coor_y_std': np.float32,\n",
    "    'screen_coor_x_std': np.float32,\n",
    "    'screen_coor_y_std': np.float32,\n",
    "    'hover_duration_std': np.float32,\n",
    "    'difference_clicks_std': np.float32,\n",
    "    'index_sum_of_actions': np.float32,\n",
    "    'difference_clicks_max': np.float32,\n",
    "    'elapsed_time_max': np.float32,\n",
    "    'clicks_per_second': np.float32}\n",
    "\n",
    "dataset_df = pd.read_csv(\"data/processed/dataset_df_level.csv\", dtype=dtypes)\n",
    "\n",
    "#load the label dataset\n",
    "labels = pd.read_csv(\"data/processed/labels.csv\")\n",
    "dataset_df_0_4 = pd.read_csv(\"data/processed/df_0_4.csv\", dtype=dtypes, index_col= 0)\n",
    "dataset_df_0_4 = dataset_df_0_4.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, test_ratio=0.20):\n",
    "    USER_LIST = dataset_df.index.unique()\n",
    "    split = int(len(USER_LIST) * (1 - 0.20))\n",
    "    return dataset.loc[USER_LIST[:split]], dataset.loc[USER_LIST[split:]]\n",
    "\n",
    "train_x, valid_x = split_dataset(dataset_df)\n",
    "print(\"{} examples in training, {} examples in testing.\".format(\n",
    "    len(train_x), len(valid_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the unique list of user sessions in the validation dataset. We assigned \n",
    "# `session_id` as the index of our feature engineered dataset. Hence fetching \n",
    "# the unique values in the index column will give us a list of users in the \n",
    "# validation set.\n",
    "VALID_USER_LIST = valid_x.index.unique()\n",
    "\n",
    "# Create a dataframe for storing the predictions of each question for all users\n",
    "# in the validation set.\n",
    "# For this, the required size of the data frame is: \n",
    "# (no: of users in validation set  x no of questions).\n",
    "# We will initialize all the predicted values in the data frame to zero.\n",
    "# The dataframe's index column is the user `session_id`s. \n",
    "prediction_df = pd.DataFrame(data=np.zeros((len(VALID_USER_LIST),18)), index=VALID_USER_LIST)\n",
    "\n",
    "# Create an empty dictionary to store the models created for each question.\n",
    "models = {}\n",
    "\n",
    "# Create an empty dictionary to store the evaluation score for each question.\n",
    "evaluation_dict ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get labels and data together\n",
    "# Iterate through questions 1 to 18 to train models for each question, evaluate\n",
    "# the trained model and store the predicted values.\n",
    "for q_no in range(1,19):\n",
    "\n",
    "    # Select level group for the question based on the q_no.\n",
    "    if q_no<=3: grp = '0-4'\n",
    "    elif q_no<=13: grp = '5-12'\n",
    "    elif q_no<=22: grp = '13-22'\n",
    "    print(\"### q_no\", q_no, \"grp\", grp)\n",
    "    \n",
    "        \n",
    "    # Filter the rows in the datasets based on the selected level group. \n",
    "    train_df = train_x.loc[train_x.level_group == grp]\n",
    "    train_users = train_df.index.values\n",
    "    valid_df = valid_x.loc[valid_x.level_group == grp]\n",
    "    valid_users = valid_df.index.values\n",
    "\n",
    "    # Select the labels for the related q_no.\n",
    "    train_labels = labels.loc[labels.q==q_no].set_index('session').loc[train_users]\n",
    "    valid_labels = labels.loc[labels.q==q_no].set_index('session').loc[valid_users]\n",
    "\n",
    "    # Add the label to the filtered datasets.\n",
    "    train_df[\"correct\"] = train_labels[\"correct\"]\n",
    "    valid_df[\"correct\"] = valid_labels[\"correct\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_rows(df, n_flatten=5, only_one=None, drop=None):\n",
    "    \"\"\"\n",
    "    Combines every n_flatten rows of a Pandas DataFrame into a new DataFrame, with each row containing the combined values from the n_flatten rows.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to combine.\n",
    "        n_flatten (int): The number of rows to be combined into a single row.\n",
    "        only_one (list): A list of column names to keep only the first occurrence of in the output DataFrame.\n",
    "        drop (list): A list of column names to drop from the input DataFrame before performing the calculation.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A new DataFrame containing one row for every n_flatten rows of the input DataFrame, with each row containing the combined values from the n_flatten rows.\n",
    "    \"\"\"\n",
    "    # Create a copy of the input DataFrame to modify\n",
    "    df = df.copy()\n",
    "\n",
    "    # Use value_counts() to get the count of each session_id\n",
    "    counts = df['session_id'].value_counts()\n",
    "\n",
    "    # Check if each group has the same number of rows\n",
    "    if (counts % n_flatten).any():\n",
    "        # Get the session_ids that need to be generated\n",
    "        need_generating = counts[counts < n_flatten].index.tolist()\n",
    "        num_generated_rows = 0\n",
    "        \n",
    "        # Loop through the session_ids that need to be generated\n",
    "        for session_id in need_generating:\n",
    "            # Generate a new row for this session_id\n",
    "            new_row = {\"session_id\": session_id}\n",
    "            for col in df.columns:\n",
    "                if col == \"session_id\":\n",
    "                    continue\n",
    "                elif df[col].dtype.name == \"category\":\n",
    "                    # Categorical column - set value to \"generated\"\n",
    "                    new_row[col] = \"generated\"\n",
    "                else:\n",
    "                    # Numeric column - set value to average of other values in column with the same level\n",
    "                    level_values = df.loc[df[\"session_id\"] == session_id, \"level\"].unique()\n",
    "                    for level in level_values:\n",
    "                        if level == \"generated\":\n",
    "                            continue\n",
    "                        other_values = df.loc[(df[\"session_id\"] == session_id) & (df[\"level\"] == level), col]\n",
    "                        if other_values.dtype.kind in 'biufc':\n",
    "                            new_value = other_values.mean()\n",
    "                            new_row[col] = new_value\n",
    "                            break\n",
    "            df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            num_generated_rows += 1\n",
    "\n",
    "        print(f\"Generated {num_generated_rows} rows with indices: {list(range(len(df) - num_generated_rows, len(df)))}\\n{new_row}\")\n",
    "\n",
    "\n",
    "    # Drop specified columns from input DataFrame\n",
    "    if drop:\n",
    "        df = df.drop(columns=drop)\n",
    "\n",
    "    # Determine the number of rows and columns in the input DataFrame\n",
    "    num_rows, num_cols = df.shape\n",
    "\n",
    "    # Determine the number of new rows in the output DataFrame\n",
    "    num_new_rows = num_rows // n_flatten\n",
    "\n",
    "    # Reshape the flattened values into a new array with the desired shape\n",
    "    values = df.values.flatten()\n",
    "    new_values = values.reshape(num_new_rows, n_flatten*num_cols)\n",
    "\n",
    "    # Create a new DataFrame from the reshaped values\n",
    "    new_df = pd.DataFrame(new_values, columns=[f\"{col}_{i}\" for i in range(1, n_flatten+1) for col in df.columns])\n",
    "\n",
    "    # Drop specified columns from output DataFrame\n",
    "    if only_one:\n",
    "        for col in only_one:\n",
    "            keep_col = f\"{col}_1\"\n",
    "            drop_cols = [f\"{col}_{i}\" for i in range(2, n_flatten+1)]\n",
    "            new_df = new_df.drop(columns=drop_cols)\n",
    "\n",
    "    return new_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'num_generated_rows' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mn:\\MASTER_DS\\Code\\Kaggle_competition\\Kaggle-seminar\\student-performance\\notebooks\\data_with_labels_steve.ipynb Cell 8\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/data_with_labels_steve.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Example usage\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/data_with_labels_steve.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m new_df \u001b[39m=\u001b[39m combine_rows(dataset_df_0_4,n_flatten\u001b[39m=\u001b[39m \u001b[39m5\u001b[39m ,drop\u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m\"\u001b[39m], only_one\u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mlevel_group\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mmusic\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mhq\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfullscreen\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[1;32mn:\\MASTER_DS\\Code\\Kaggle_competition\\Kaggle-seminar\\student-performance\\notebooks\\data_with_labels_steve.ipynb Cell 8\u001b[0m in \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/data_with_labels_steve.ipynb#X10sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m num_rows, num_cols \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mshape\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/data_with_labels_steve.ipynb#X10sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m# Determine the number of new rows in the output DataFrame\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/data_with_labels_steve.ipynb#X10sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m num_new_rows \u001b[39m=\u001b[39m (num_rows \u001b[39m+\u001b[39m num_generated_rows) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_flatten\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/data_with_labels_steve.ipynb#X10sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# Reshape the flattened values into a new array with the desired shape\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/data_with_labels_steve.ipynb#X10sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m values \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mflatten()\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'num_generated_rows' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "new_df = combine_rows(dataset_df_0_4,n_flatten= 5 ,drop= [\"level\"], only_one= [\"level_group\",\"music\", \"hq\", \"fullscreen\"])\n",
    "#print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13_22 = pd.read_csv(\"data\\processed\\df_13_22.csv\", dtype=dtypes, index_col= 0)\n",
    "df_13_22 = df_13_22.reset_index(drop=True)\n",
    "ex = [\"level_group\",\"music\", \"hq\", \"fullscreen\"]\n",
    "drop = [\"level\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'session_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mn:\\Python\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mn:\\Python\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mn:\\Python\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'session_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mn:\\MASTER_DS\\Code\\Kaggle_competition\\Kaggle-seminar\\student-performance\\notebooks\\data_with_labels_steve.ipynb Cell 10\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/data_with_labels_steve.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_13_22 \u001b[39m=\u001b[39m combine_rows(df_13_22,n_flatten\u001b[39m=\u001b[39m \u001b[39m10\u001b[39m ,drop\u001b[39m=\u001b[39m drop, only_one\u001b[39m=\u001b[39m ex)\n",
      "\u001b[1;32mn:\\MASTER_DS\\Code\\Kaggle_competition\\Kaggle-seminar\\student-performance\\notebooks\\data_with_labels_steve.ipynb Cell 10\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/data_with_labels_steve.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/data_with_labels_steve.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Use value_counts() to get the count of each session_id\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/data_with_labels_steve.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m counts \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39msession_id\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mvalue_counts()\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/data_with_labels_steve.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Check if each group has the same number of rows\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/data_with_labels_steve.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mif\u001b[39;00m (counts \u001b[39m%\u001b[39m n_flatten)\u001b[39m.\u001b[39many():\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/data_with_labels_steve.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# Get the session_ids that need to be generated\u001b[39;00m\n",
      "File \u001b[1;32mn:\\Python\\lib\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3806\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mn:\\Python\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'session_id'"
     ]
    }
   ],
   "source": [
    "df_13_22 = combine_rows(df_13_22,n_flatten= 10 ,drop= drop, only_one= ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_rows(df, n_flatten=5, only_one=None, drop=None):\n",
    "    \"\"\"\n",
    "    Combines every n_flatten rows of a Pandas DataFrame into a new DataFrame, with each row containing the combined values from the n_flatten rows.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to combine.\n",
    "        n_flatten (int): The number of rows to be combined into a single row.\n",
    "        only_one (list): A list of column names to keep only the first occurrence of in the output DataFrame.\n",
    "        drop (list): A list of column names to drop from the input DataFrame before performing the calculation.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A new DataFrame containing one row for every n_flatten rows of the input DataFrame, with each row containing the combined values from the n_flatten rows.\n",
    "    \"\"\"\n",
    "    # Create a copy of the input DataFrame to modify\n",
    "    df = df.copy()\n",
    "\n",
    "    # Use value_counts() to get the count of each session_id\n",
    "    counts = df['session_id'].value_counts()\n",
    "\n",
    "    # Check if each group has the same number of rows\n",
    "    if (counts % n_flatten).any():\n",
    "        # Get the session_ids that need to be generated\n",
    "        need_generating = counts[counts < n_flatten].index.tolist()\n",
    "        num_generated_rows = 0\n",
    "        \n",
    "        # Loop through the session_ids that need to be generated\n",
    "        for session_id in need_generating:\n",
    "            # Generate new rows for this session_id\n",
    "            num_missing_rows = n_flatten - counts[session_id]\n",
    "            new_rows = []\n",
    "            for i in range(num_missing_rows):\n",
    "                new_row = {\"session_id\": session_id}\n",
    "                for col in df.columns:\n",
    "                    if col == \"session_id\":\n",
    "                        continue\n",
    "                    elif df[col].dtype.name == \"category\":\n",
    "                        # Categorical column - set value to \"generated\"\n",
    "                        new_row[col] = \"generated\"\n",
    "                    else:\n",
    "                        # Numeric column - set value to average of other values in column with the same level\n",
    "                        level_values = df.loc[df[\"session_id\"] == session_id, \"level\"].unique()\n",
    "                        for level in level_values:\n",
    "                            if level == \"generated\":\n",
    "                                continue\n",
    "                            other_values = df.loc[(df[\"session_id\"] == session_id) & (df[\"level\"] == level), col]\n",
    "                            if other_values.dtype.kind in 'biufc':\n",
    "                                new_value = other_values.mean()\n",
    "                                new_row[col] = new_value\n",
    "                                break\n",
    "                new_rows.append(new_row)\n",
    "            df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "            num_generated_rows += num_missing_rows\n",
    "\n",
    "        print(f\"Generated {num_generated_rows} rows with indices: {list(range(len(df) - num_generated_rows, len(df)))}\\n{new_rows}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Drop specified columns from input DataFrame\n",
    "    if drop:\n",
    "        df = df.drop(columns=drop)\n",
    "\n",
    "    # Determine the number of rows and columns in the input DataFrame\n",
    "    num_rows, num_cols = df.shape\n",
    "\n",
    "    # Determine the number of new rows in the output DataFrame\n",
    "    num_new_rows = (num_rows + num_generated_rows) // n_flatten\n",
    "\n",
    "    # Reshape the flattened values into a new array with the desired shape\n",
    "    values = df.values.flatten()\n",
    "    new_values = values.reshape(num_new_rows, n_flatten*num_cols)\n",
    "\n",
    "    # Create a new DataFrame from the reshaped values\n",
    "    new_df = pd.DataFrame(new_values, columns=[f\"{col}_{i}\" for i in range(1, n_flatten+1) for col in df.columns])\n",
    "\n",
    "    # Drop specified columns from output DataFrame\n",
    "    if only_one:\n",
    "        for col in only_one:\n",
    "            keep_col = f\"{col}_1\"\n",
    "            drop_cols = [f\"{col}_{i}\" for i in range(2, n_flatten+1)]\n",
    "            new_df = new_df.drop(columns=drop_cols)\n",
    "\n",
    "    return new_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
