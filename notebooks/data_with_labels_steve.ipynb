{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports for the Data Preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import os\n",
    "import gc\n",
    "from typing import Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory:  n:\\MASTER_DS\\Code\\Kaggle_competition\\Kaggle-seminar\\student-performance\n"
     ]
    }
   ],
   "source": [
    "# get working directory and remove last folder\n",
    "# TODO: make this more robust\n",
    "wd = os.path.dirname(os.getcwd())\n",
    "os.chdir(wd)\n",
    "print('Working Directory: ', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'level': np.uint8,\n",
    "    \"level_group\": \"category\",\n",
    "    'event_name': np.uint8,\n",
    "    'name': np.uint8,\n",
    "    'fqid': np.uint8,\n",
    "    'room_fqid': np.uint8,           \n",
    "    \"text_fqid\": np.uint8,\n",
    "    'fullscreen': np.uint8,\n",
    "    'hq': np.uint8,\n",
    "    'music': np.uint8,\n",
    "    'hover_duration_mean': np.float32,\n",
    "    'difference_clicks_mean': np.float32,\n",
    "    'elapsed_time_std': np.float32,\n",
    "    'page_std': np.float32,\n",
    "    'room_coor_x_std': np.float32,\n",
    "    'room_coor_y_std': np.float32,\n",
    "    'screen_coor_x_std': np.float32,\n",
    "    'screen_coor_y_std': np.float32,\n",
    "    'hover_duration_std': np.float32,\n",
    "    'difference_clicks_std': np.float32,\n",
    "    'index_sum_of_actions': np.float32,\n",
    "    'difference_clicks_max': np.float32,\n",
    "    'elapsed_time_max': np.float32,\n",
    "    'clicks_per_second': np.float32}\n",
    "\n",
    "dataset_df = pd.read_csv(\"data/processed/dataset_df_level.csv\", dtype=dtypes)\n",
    "\n",
    "#load the label dataset\n",
    "#labels = pd.read_csv(\"data/processed/labels.csv\")\n",
    "dataset_df_0_4 = pd.read_csv(\"data/processed/df_0_4.csv\", dtype=dtypes, index_col= 0)\n",
    "dataset_df_0_4 = dataset_df_0_4.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, test_ratio=0.20):\n",
    "    USER_LIST = dataset_df.index.unique()\n",
    "    split = int(len(USER_LIST) * (1 - 0.20))\n",
    "    return dataset.loc[USER_LIST[:split]], dataset.loc[USER_LIST[split:]]\n",
    "\n",
    "train_x, valid_x = split_dataset(dataset_df)\n",
    "print(\"{} examples in training, {} examples in testing.\".format(\n",
    "    len(train_x), len(valid_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the unique list of user sessions in the validation dataset. We assigned \n",
    "# `session_id` as the index of our feature engineered dataset. Hence fetching \n",
    "# the unique values in the index column will give us a list of users in the \n",
    "# validation set.\n",
    "VALID_USER_LIST = valid_x.index.unique()\n",
    "\n",
    "# Create a dataframe for storing the predictions of each question for all users\n",
    "# in the validation set.\n",
    "# For this, the required size of the data frame is: \n",
    "# (no: of users in validation set  x no of questions).\n",
    "# We will initialize all the predicted values in the data frame to zero.\n",
    "# The dataframe's index column is the user `session_id`s. \n",
    "prediction_df = pd.DataFrame(data=np.zeros((len(VALID_USER_LIST),18)), index=VALID_USER_LIST)\n",
    "\n",
    "# Create an empty dictionary to store the models created for each question.\n",
    "models = {}\n",
    "\n",
    "# Create an empty dictionary to store the evaluation score for each question.\n",
    "evaluation_dict ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get labels and data together\n",
    "# Iterate through questions 1 to 18 to train models for each question, evaluate\n",
    "# the trained model and store the predicted values.\n",
    "for q_no in range(1,19):\n",
    "\n",
    "    # Select level group for the question based on the q_no.\n",
    "    if q_no<=3: grp = '0-4'\n",
    "    elif q_no<=13: grp = '5-12'\n",
    "    elif q_no<=22: grp = '13-22'\n",
    "    print(\"### q_no\", q_no, \"grp\", grp)\n",
    "    \n",
    "        \n",
    "    # Filter the rows in the datasets based on the selected level group. \n",
    "    train_df = train_x.loc[train_x.level_group == grp]\n",
    "    train_users = train_df.index.values\n",
    "    valid_df = valid_x.loc[valid_x.level_group == grp]\n",
    "    valid_users = valid_df.index.values\n",
    "\n",
    "    # Select the labels for the related q_no.\n",
    "    train_labels = labels.loc[labels.q==q_no].set_index('session').loc[train_users]\n",
    "    valid_labels = labels.loc[labels.q==q_no].set_index('session').loc[valid_users]\n",
    "\n",
    "    # Add the label to the filtered datasets.\n",
    "    train_df[\"correct\"] = train_labels[\"correct\"]\n",
    "    valid_df[\"correct\"] = valid_labels[\"correct\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outdated\n",
    "'''def combine_rows(df, n_flatten=5, only_one=None, drop=None):\n",
    "    \"\"\"\n",
    "    Combines every n_flatten rows of a Pandas DataFrame into a new DataFrame, with each row containing the combined values from the n_flatten rows.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to combine.\n",
    "        n_flatten (int): The number of rows to be combined into a single row.\n",
    "        only_one (list): A list of column names to keep only the first occurrence of in the output DataFrame.\n",
    "        drop (list): A list of column names to drop from the input DataFrame before performing the calculation.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A new DataFrame containing one row for every n_flatten rows of the input DataFrame, with each row containing the combined values from the n_flatten rows.\n",
    "    \"\"\"\n",
    "    # Create a copy of the input DataFrame to modify\n",
    "    df = df.copy()\n",
    "\n",
    "    # Use value_counts() to get the count of each session_id\n",
    "    counts = df['session_id'].value_counts()\n",
    "\n",
    "    # Check if each group has the same number of rows\n",
    "    if (counts % n_flatten).any():\n",
    "        # Get the session_ids that need to be generated\n",
    "        need_generating = counts[counts < n_flatten].index.tolist()\n",
    "        num_generated_rows = 0\n",
    "        \n",
    "        # Loop through the session_ids that need to be generated\n",
    "        for session_id in need_generating:\n",
    "            # Generate a new row for this session_id\n",
    "            new_row = {\"session_id\": session_id}\n",
    "            for col in df.columns:\n",
    "                if col == \"session_id\":\n",
    "                    continue\n",
    "                elif df[col].dtype.name == \"category\":\n",
    "                    # Categorical column - set value to \"generated\"\n",
    "                    new_row[col] = \"generated\"\n",
    "                else:\n",
    "                    # Numeric column - set value to average of other values in column with the same level\n",
    "                    level_values = df.loc[df[\"session_id\"] == session_id, \"level\"].unique()\n",
    "                    for level in level_values:\n",
    "                        if level == \"generated\":\n",
    "                            continue\n",
    "                        other_values = df.loc[(df[\"session_id\"] == session_id) & (df[\"level\"] == level), col]\n",
    "                        if other_values.dtype.kind in 'biufc':\n",
    "                            new_value = other_values.mean()\n",
    "                            new_row[col] = new_value\n",
    "                            break\n",
    "            df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            num_generated_rows += 1\n",
    "\n",
    "        print(f\"Generated {num_generated_rows} rows with indices: {list(range(len(df) - num_generated_rows, len(df)))}\\n{new_row}\")\n",
    "\n",
    "\n",
    "    # Drop specified columns from input DataFrame\n",
    "    if drop:\n",
    "        df = df.drop(columns=drop)\n",
    "\n",
    "    # Determine the number of rows and columns in the input DataFrame\n",
    "    num_rows, num_cols = df.shape\n",
    "\n",
    "    # Determine the number of new rows in the output DataFrame\n",
    "    num_new_rows = num_rows // n_flatten\n",
    "\n",
    "    # Reshape the flattened values into a new array with the desired shape\n",
    "    values = df.values.flatten()\n",
    "    new_values = values.reshape(num_new_rows, n_flatten*num_cols)\n",
    "\n",
    "    # Create a new DataFrame from the reshaped values\n",
    "    new_df = pd.DataFrame(new_values, columns=[f\"{col}_{i}\" for i in range(1, n_flatten+1) for col in df.columns])\n",
    "\n",
    "    # Drop specified columns from output DataFrame\n",
    "    if only_one:\n",
    "        for col in only_one:\n",
    "            keep_col = f\"{col}_1\"\n",
    "            drop_cols = [f\"{col}_{i}\" for i in range(2, n_flatten+1)]\n",
    "            new_df = new_df.drop(columns=drop_cols)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "new_df = combine_rows(df_13_22 ,n_flatten= 10 ,drop= [\"level\"], only_one= [\"level_group\",\"music\", \"hq\", \"fullscreen\"])\n",
    "#print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13_22 = pd.read_csv(\"data\\processed\\df_13_22.csv\", index_col= 0, nrows= 200000, dtype= dtypes)\n",
    "df_13_22 = df_13_22.reset_index(drop=True)\n",
    "ex = [\"level_group\",\"music\", \"hq\", \"fullscreen\"]\n",
    "drop = [\"level\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing level: All session_id's should occur the same amount of times.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mn:\\MASTER_DS\\Code\\Kaggle_competition\\Kaggle-seminar\\student-performance\\notebooks\\data_with_labels_steve.ipynb Cell 11\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/data_with_labels_steve.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_13_22 \u001b[39m=\u001b[39m combine_rows(df_13_22,n_flatten\u001b[39m=\u001b[39m \u001b[39m10\u001b[39m ,drop\u001b[39m=\u001b[39m drop, only_one\u001b[39m=\u001b[39m ex)\n",
      "\u001b[1;32mn:\\MASTER_DS\\Code\\Kaggle_competition\\Kaggle-seminar\\student-performance\\notebooks\\data_with_labels_steve.ipynb Cell 11\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/data_with_labels_steve.ipynb#X30sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m session_counts \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39msession_id\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalue_counts()\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/data_with_labels_steve.ipynb#X30sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(session_counts\u001b[39m.\u001b[39mvalues)) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/data_with_labels_steve.ipynb#X30sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMissing level: All session_id\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms should occur the same amount of times.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/data_with_labels_steve.ipynb#X30sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Determine the number of rows and columns in the input DataFrame\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/notebooks/data_with_labels_steve.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m num_rows, num_cols \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mValueError\u001b[0m: Missing level: All session_id's should occur the same amount of times."
     ]
    }
   ],
   "source": [
    "df_13_22 = combine_rows(df_13_22,n_flatten= 10 ,drop= drop, only_one= ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2892646847.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [12], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"\"\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#outdated\n",
    "'''def combine_rows(df, n_flatten=5, only_one=None, drop=None):\n",
    "    \"\"\"\n",
    "    Combines every n_flatten rows of a Pandas DataFrame into a new DataFrame, with each row containing the combined values from the n_flatten rows.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to combine.\n",
    "        n_flatten (int): The number of rows to be combined into a single row.\n",
    "        only_one (list): A list of column names to keep only the first occurrence of in the output DataFrame.\n",
    "        drop (list): A list of column names to drop from the input DataFrame before performing the calculation.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A new DataFrame containing one row for every n_flatten rows of the input DataFrame, with each row containing the combined values from the n_flatten rows.\n",
    "    \"\"\"\n",
    "    # Create a copy of the input DataFrame to modify\n",
    "    df = df.copy()\n",
    "\n",
    "    # Use value_counts() to get the count of each session_id\n",
    "    counts = df['session_id'].value_counts()\n",
    "\n",
    "    # Check if each group has the same number of rows\n",
    "    if (counts % n_flatten).any():\n",
    "        # Get the session_ids that need to be generated\n",
    "        need_generating = counts[counts < n_flatten].index.tolist()\n",
    "        num_generated_rows = 0\n",
    "        \n",
    "        # Loop through the session_ids that need to be generated\n",
    "        for session_id in need_generating:\n",
    "            # Generate new rows for this session_id\n",
    "            num_missing_rows = n_flatten - counts[session_id]\n",
    "            new_rows = []\n",
    "            for i in range(num_missing_rows):\n",
    "                new_row = {\"session_id\": session_id}\n",
    "                for col in df.columns:\n",
    "                    if col == \"session_id\":\n",
    "                        continue\n",
    "                    elif df[col].dtype.name == \"category\":\n",
    "                        # Categorical column - set value to \"generated\"\n",
    "                        new_row[col] = \"generated\"\n",
    "                    else:\n",
    "                        # Numeric column - set value to average of other values in column with the same level\n",
    "                        level_values = df.loc[df[\"session_id\"] == session_id, \"level\"].unique()\n",
    "                        for level in level_values:\n",
    "                            if level == \"generated\":\n",
    "                                continue\n",
    "                            other_values = df.loc[(df[\"session_id\"] == session_id) & (df[\"level\"] == level), col]\n",
    "                            if other_values.dtype.kind in 'biufc':\n",
    "                                new_value = other_values.mean()\n",
    "                                new_row[col] = new_value\n",
    "                                break\n",
    "                new_rows.append(new_row)\n",
    "            df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "            num_generated_rows += num_missing_rows\n",
    "\n",
    "        print(f\"Generated {num_generated_rows} rows with indices: {list(range(len(df) - num_generated_rows, len(df)))}\\n{new_rows}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Drop specified columns from input DataFrame\n",
    "    if drop:\n",
    "        df = df.drop(columns=drop)\n",
    "\n",
    "    # Determine the number of rows and columns in the input DataFrame\n",
    "    num_rows, num_cols = df.shape\n",
    "\n",
    "    # Determine the number of new rows in the output DataFrame\n",
    "    num_new_rows = (num_rows + num_generated_rows) // n_flatten\n",
    "\n",
    "    # Reshape the flattened values into a new array with the desired shape\n",
    "    values = df.values.flatten()\n",
    "    new_values = values.reshape(num_new_rows, n_flatten*num_cols)\n",
    "\n",
    "    # Create a new DataFrame from the reshaped values\n",
    "    new_df = pd.DataFrame(new_values, columns=[f\"{col}_{i}\" for i in range(1, n_flatten+1) for col in df.columns])\n",
    "\n",
    "    # Drop specified columns from output DataFrame\n",
    "    if only_one:\n",
    "        for col in only_one:\n",
    "            keep_col = f\"{col}_1\"\n",
    "            drop_cols = [f\"{col}_{i}\" for i in range(2, n_flatten+1)]\n",
    "            new_df = new_df.drop(columns=drop_cols)\n",
    "\n",
    "    return new_df'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rows(df: pd.DataFrame, n_flatten: int, level_g: str):\n",
    "    # Use value_counts() to get the count of each session_id\n",
    "    counts = df['session_id'].value_counts()\n",
    "\n",
    "    # Check if each group has the same number of rows\n",
    "    if (counts % n_flatten).any():\n",
    "        # Get the session_ids that need to be generated\n",
    "        need_generating = counts[counts < n_flatten].index.tolist()\n",
    "        num_generated_rows = 0\n",
    "        generated_sessions = []\n",
    "        generated_rows = []\n",
    "\n",
    "        # Loop through the session_ids that need to be generated\n",
    "        for session_id in need_generating:\n",
    "            # Check if all levels are present in this session\n",
    "            levels_present = set(df.loc[df['session_id'] == session_id, 'level'].unique())\n",
    "            min_level = df['level'].min()\n",
    "            max_level = df['level'].max()\n",
    "            expected_levels = set(range(min_level, max_level + 1))\n",
    "            if levels_present != expected_levels:\n",
    "                # Generate new rows for missing levels\n",
    "                missing_levels = expected_levels - levels_present\n",
    "                new_rows = []\n",
    "                for missing_level in missing_levels:\n",
    "                    new_row = {\"session_id\": session_id, \"level\": missing_level}\n",
    "                    for col in df.columns:\n",
    "                        if col == \"session_id\" or col == \"level\":\n",
    "                            continue\n",
    "                        \n",
    "                        else:\n",
    "                            # Numeric column - set value to average of other values in column with the same level\n",
    "                            other_values = df.loc[(df[\"session_id\"] == session_id) & (df[\"level\"] == missing_level), col]\n",
    "                            if other_values.dtype.kind in 'biufc':\n",
    "                                new_value = other_values.mean()\n",
    "                                if np.isnan(new_value):\n",
    "                                    new_value = df.loc[df[\"level\"] == missing_level, col].mean()\n",
    "                                new_row[col] = new_value\n",
    "                    new_rows.append(new_row)\n",
    "\n",
    "                # Add the new rows to the dataframe\n",
    "                df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "                num_generated_rows += len(new_rows)\n",
    "                generated_sessions.append({\"session_id\": session_id, \"num_rows\": len(new_rows)})\n",
    "                generated_rows.extend(new_rows)\n",
    "\n",
    "        print(f\"Generated {num_generated_rows} rows with indices: {list(range(len(df) - num_generated_rows, len(df)))}\")\n",
    "\n",
    "        # Create output dataframe 2\n",
    "        df2 = pd.DataFrame(generated_sessions)\n",
    "        df2 = df2.set_index(\"session_id\")\n",
    "        print(\"Generated rows per session id:\")\n",
    "        print(df2)\n",
    "\n",
    "        # Create output dataframe 3\n",
    "        df3 = pd.DataFrame(generated_rows)\n",
    "        df3 = df3.reindex(df.columns, axis=1)\n",
    "        print(\"Generated rows:\")\n",
    "        print(df3)\n",
    "    else:\n",
    "        df2 = pd.DataFrame()\n",
    "        df3 = pd.DataFrame()\n",
    "    df[\"level_group\"] = level_g\n",
    "    df3[\"level_group\"] = level_g\n",
    "    return df, df2, df3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 500 rows with indices: [200000, 200001, 200002, 200003, 200004, 200005, 200006, 200007, 200008, 200009, 200010, 200011, 200012, 200013, 200014, 200015, 200016, 200017, 200018, 200019, 200020, 200021, 200022, 200023, 200024, 200025, 200026, 200027, 200028, 200029, 200030, 200031, 200032, 200033, 200034, 200035, 200036, 200037, 200038, 200039, 200040, 200041, 200042, 200043, 200044, 200045, 200046, 200047, 200048, 200049, 200050, 200051, 200052, 200053, 200054, 200055, 200056, 200057, 200058, 200059, 200060, 200061, 200062, 200063, 200064, 200065, 200066, 200067, 200068, 200069, 200070, 200071, 200072, 200073, 200074, 200075, 200076, 200077, 200078, 200079, 200080, 200081, 200082, 200083, 200084, 200085, 200086, 200087, 200088, 200089, 200090, 200091, 200092, 200093, 200094, 200095, 200096, 200097, 200098, 200099, 200100, 200101, 200102, 200103, 200104, 200105, 200106, 200107, 200108, 200109, 200110, 200111, 200112, 200113, 200114, 200115, 200116, 200117, 200118, 200119, 200120, 200121, 200122, 200123, 200124, 200125, 200126, 200127, 200128, 200129, 200130, 200131, 200132, 200133, 200134, 200135, 200136, 200137, 200138, 200139, 200140, 200141, 200142, 200143, 200144, 200145, 200146, 200147, 200148, 200149, 200150, 200151, 200152, 200153, 200154, 200155, 200156, 200157, 200158, 200159, 200160, 200161, 200162, 200163, 200164, 200165, 200166, 200167, 200168, 200169, 200170, 200171, 200172, 200173, 200174, 200175, 200176, 200177, 200178, 200179, 200180, 200181, 200182, 200183, 200184, 200185, 200186, 200187, 200188, 200189, 200190, 200191, 200192, 200193, 200194, 200195, 200196, 200197, 200198, 200199, 200200, 200201, 200202, 200203, 200204, 200205, 200206, 200207, 200208, 200209, 200210, 200211, 200212, 200213, 200214, 200215, 200216, 200217, 200218, 200219, 200220, 200221, 200222, 200223, 200224, 200225, 200226, 200227, 200228, 200229, 200230, 200231, 200232, 200233, 200234, 200235, 200236, 200237, 200238, 200239, 200240, 200241, 200242, 200243, 200244, 200245, 200246, 200247, 200248, 200249, 200250, 200251, 200252, 200253, 200254, 200255, 200256, 200257, 200258, 200259, 200260, 200261, 200262, 200263, 200264, 200265, 200266, 200267, 200268, 200269, 200270, 200271, 200272, 200273, 200274, 200275, 200276, 200277, 200278, 200279, 200280, 200281, 200282, 200283, 200284, 200285, 200286, 200287, 200288, 200289, 200290, 200291, 200292, 200293, 200294, 200295, 200296, 200297, 200298, 200299, 200300, 200301, 200302, 200303, 200304, 200305, 200306, 200307, 200308, 200309, 200310, 200311, 200312, 200313, 200314, 200315, 200316, 200317, 200318, 200319, 200320, 200321, 200322, 200323, 200324, 200325, 200326, 200327, 200328, 200329, 200330, 200331, 200332, 200333, 200334, 200335, 200336, 200337, 200338, 200339, 200340, 200341, 200342, 200343, 200344, 200345, 200346, 200347, 200348, 200349, 200350, 200351, 200352, 200353, 200354, 200355, 200356, 200357, 200358, 200359, 200360, 200361, 200362, 200363, 200364, 200365, 200366, 200367, 200368, 200369, 200370, 200371, 200372, 200373, 200374, 200375, 200376, 200377, 200378, 200379, 200380, 200381, 200382, 200383, 200384, 200385, 200386, 200387, 200388, 200389, 200390, 200391, 200392, 200393, 200394, 200395, 200396, 200397, 200398, 200399, 200400, 200401, 200402, 200403, 200404, 200405, 200406, 200407, 200408, 200409, 200410, 200411, 200412, 200413, 200414, 200415, 200416, 200417, 200418, 200419, 200420, 200421, 200422, 200423, 200424, 200425, 200426, 200427, 200428, 200429, 200430, 200431, 200432, 200433, 200434, 200435, 200436, 200437, 200438, 200439, 200440, 200441, 200442, 200443, 200444, 200445, 200446, 200447, 200448, 200449, 200450, 200451, 200452, 200453, 200454, 200455, 200456, 200457, 200458, 200459, 200460, 200461, 200462, 200463, 200464, 200465, 200466, 200467, 200468, 200469, 200470, 200471, 200472, 200473, 200474, 200475, 200476, 200477, 200478, 200479, 200480, 200481, 200482, 200483, 200484, 200485, 200486, 200487, 200488, 200489, 200490, 200491, 200492, 200493, 200494, 200495, 200496, 200497, 200498, 200499]\n",
      "Generated rows per session id:\n",
      "                   num_rows\n",
      "session_id                 \n",
      "21070411113995976         1\n",
      "21030314101487556         1\n",
      "21000513504164910         1\n",
      "21100210000283590         1\n",
      "21070314452027028         1\n",
      "...                     ...\n",
      "21030207240628590         2\n",
      "21020108583469690         2\n",
      "20110410072633920         2\n",
      "22010218425968836         2\n",
      "22020315233317980         2\n",
      "\n",
      "[493 rows x 1 columns]\n",
      "Generated rows:\n",
      "            session_id  level  level_group  event_name      name       fqid  \\\n",
      "0    21070411113995976     15          NaN    5.967200  3.385480   9.577990   \n",
      "1    21030314101487556     15          NaN    5.967200  3.385480   9.577990   \n",
      "2    21000513504164910     15          NaN    5.967200  3.385480   9.577990   \n",
      "3    21100210000283590     15          NaN    5.967200  3.385480   9.577990   \n",
      "4    21070314452027028     15          NaN    5.967200  3.385480   9.577990   \n",
      "..                 ...    ...          ...         ...       ...        ...   \n",
      "495  20110410072633920     15          NaN    5.967200  3.385480   9.577990   \n",
      "496  22010218425968836     21          NaN    7.376740  3.546276  16.847278   \n",
      "497  22010218425968836     15          NaN    5.967200  3.385480   9.577990   \n",
      "498  22020315233317980     21          NaN    7.376740  3.546276  16.847278   \n",
      "499  22020315233317980     22          NaN    4.335528  2.648561   6.097312   \n",
      "\n",
      "     room_fqid  text_fqid  fullscreen        hq  ...  room_coor_x_std  \\\n",
      "0     5.471159   3.552956    0.132427  0.120523  ...       634.652283   \n",
      "1     5.471159   3.552956    0.132427  0.120523  ...       634.652283   \n",
      "2     5.471159   3.552956    0.132427  0.120523  ...       634.652283   \n",
      "3     5.471159   3.552956    0.132427  0.120523  ...       634.652283   \n",
      "4     5.471159   3.552956    0.132427  0.120523  ...       634.652283   \n",
      "..         ...        ...         ...       ...  ...              ...   \n",
      "495   5.471159   3.552956    0.132427  0.120523  ...       634.652283   \n",
      "496   5.957342   4.601008    0.132116  0.120591  ...       342.637360   \n",
      "497   5.471159   3.552956    0.132427  0.120523  ...       634.652283   \n",
      "498   5.957342   4.601008    0.132116  0.120591  ...       342.637360   \n",
      "499   3.380917   0.158362    0.132076  0.120605  ...       408.074554   \n",
      "\n",
      "     room_coor_y_std  screen_coor_x_std  screen_coor_y_std  \\\n",
      "0         182.327713         252.814285         133.582352   \n",
      "1         182.327698         252.814270         133.582336   \n",
      "2         182.327713         252.814285         133.582336   \n",
      "3         182.327713         252.814255         133.582352   \n",
      "4         182.327713         252.814270         133.582336   \n",
      "..               ...                ...                ...   \n",
      "495       182.327713         252.814270         133.582336   \n",
      "496       149.787216         228.913803         129.894424   \n",
      "497       182.327713         252.814285         133.582352   \n",
      "498       149.787201         228.913818         129.894440   \n",
      "499       163.484039         241.517059         146.986465   \n",
      "\n",
      "     hover_duration_std  difference_clicks_std  index_sum_of_actions  \\\n",
      "0           2118.188721              45.669437             57.766514   \n",
      "1           2118.188721              45.669437             57.766514   \n",
      "2           2118.188721              45.669437             57.766514   \n",
      "3           2118.188721              45.669437             57.766514   \n",
      "4           2118.188965              45.669437             57.766510   \n",
      "..                  ...                    ...                   ...   \n",
      "495         2118.188721              45.669437             57.766518   \n",
      "496         3031.324707              44.788883             94.699547   \n",
      "497         2118.188721              45.669437             57.766518   \n",
      "498         3031.324707              44.788887             94.699547   \n",
      "499          687.737305              22.902876             21.253180   \n",
      "\n",
      "     difference_clicks_max  elapsed_time_max  clicks_per_second  \n",
      "0               369.973572        431.743103           0.816970  \n",
      "1               369.973602        431.743134           0.816970  \n",
      "2               369.973602        431.743103           0.816970  \n",
      "3               369.973602        431.743103           0.816970  \n",
      "4               369.973602        431.743103           0.816970  \n",
      "..                     ...               ...                ...  \n",
      "495             369.973602        431.743103           0.816970  \n",
      "496             503.410309        687.414856           0.837173  \n",
      "497             369.973602        431.743134           0.816970  \n",
      "498             503.410339        687.414856           0.837173  \n",
      "499             129.325226        145.962585           0.959496  \n",
      "\n",
      "[500 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "df_13_22_gen, generated, rows_df = generate_rows(df_13_22, n_flatten= 10, level_g = \"13-22\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1,collected,rowss = generate_rows(dataset_df_0_4, n_flatten= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_rows(df, n_flatten=5, only_one=None, drop=None):\n",
    "    \"\"\"\n",
    "    Combines every n_flatten rows of a Pandas DataFrame into a new DataFrame, with each row containing the combined values from the n_flatten rows.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to combine.\n",
    "        n_flatten (int): The number of rows to be combined into a single row.\n",
    "        only_one (list): A list of column names to keep only the first occurrence of in the output DataFrame.\n",
    "        drop (list): A list of column names to drop from the input DataFrame before performing the calculation.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A new DataFrame containing one row for every n_flatten rows of the input DataFrame, with each row containing the combined values from the n_flatten rows.\n",
    "    \"\"\"\n",
    "    # Create a copy of the input DataFrame to modify\n",
    "    df = df.copy()\n",
    "\n",
    "    # Drop specified columns from input DataFrame\n",
    "    if drop:\n",
    "        df = df.drop(columns=drop)\n",
    "\n",
    "    # Check if all session_id's occur the same amount of times\n",
    "    session_counts = df['session_id'].value_counts()\n",
    "    if len(set(session_counts.values)) > 1:\n",
    "        raise ValueError(\"Missing level: All session_id's should occur the same amount of times.\")\n",
    "\n",
    "    # Determine the number of rows and columns in the input DataFrame\n",
    "    num_rows, num_cols = df.shape\n",
    "\n",
    "    # Determine the number of new rows in the output DataFrame\n",
    "    num_new_rows = num_rows // n_flatten\n",
    "\n",
    "    # Reshape the flattened values into a new array with the desired shape\n",
    "    values = df.values.flatten()\n",
    "    new_values = values.reshape(num_new_rows, n_flatten*num_cols)\n",
    "\n",
    "    # Create a new DataFrame from the reshaped values\n",
    "    new_df = pd.DataFrame(new_values, columns=[f\"{col}_{i}\" for i in range(1, n_flatten+1) for col in df.columns])\n",
    "\n",
    "    # Drop specified columns from output DataFrame\n",
    "    if only_one:\n",
    "        for col in only_one:\n",
    "            keep_col = f\"{col}_1\"\n",
    "            drop_cols = [f\"{col}_{i}\" for i in range(2, n_flatten+1)]\n",
    "            new_df = new_df.drop(columns=drop_cols)\n",
    "\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalDtype(categories=['13-22'], ordered=False)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_13_22[\"level_group\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_4_gen, generated, rows_df = generate_rows(dataset_df_0_4, n_flatten= 5, level_g = \"0-4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
