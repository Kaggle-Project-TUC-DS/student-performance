{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports for the Data Preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import os\n",
    "import gc\n",
    "from typing import Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get working directory and remove last folder\n",
    "# TODO: make this more robust\n",
    "wd = os.path.dirname(os.getcwd())\n",
    "os.chdir(wd)\n",
    "print('Working Directory: ', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'level': np.uint8,\n",
    "    \"level_group\": \"category\",\n",
    "    'event_name': np.uint8,\n",
    "    'name': np.uint8,\n",
    "    'fqid': np.uint8,\n",
    "    'room_fqid': np.uint8,           \n",
    "    \"text_fqid\": np.uint8,\n",
    "    'fullscreen': np.uint8,\n",
    "    'hq': np.uint8,\n",
    "    'music': np.uint8,\n",
    "    'hover_duration_mean': np.float32,\n",
    "    'difference_clicks_mean': np.float32,\n",
    "    'elapsed_time_std': np.float32,\n",
    "    'page_std': np.float32,\n",
    "    'room_coor_x_std': np.float32,\n",
    "    'room_coor_y_std': np.float32,\n",
    "    'screen_coor_x_std': np.float32,\n",
    "    'screen_coor_y_std': np.float32,\n",
    "    'hover_duration_std': np.float32,\n",
    "    'difference_clicks_std': np.float32,\n",
    "    'index_sum_of_actions': np.float32,\n",
    "    'difference_clicks_max': np.float32,\n",
    "    'elapsed_time_max': np.float32,\n",
    "    'clicks_per_second': np.float32}\n",
    "\n",
    "dataset_df = pd.read_csv(\"data/processed/dataset_df_level.csv\", dtype=dtypes)\n",
    "\n",
    "#load the label dataset\n",
    "#labels = pd.read_csv(\"data/processed/labels.csv\")\n",
    "#dataset_df_0_4 = pd.read_csv(\"data/processed/df_0_4.csv\", dtype=dtypes, index_col= 0)\n",
    "#dataset_df_0_4 = dataset_df_0_4.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, test_ratio=0.20):\n",
    "    USER_LIST = dataset_df.index.unique()\n",
    "    split = int(len(USER_LIST) * (1 - 0.20))\n",
    "    return dataset.loc[USER_LIST[:split]], dataset.loc[USER_LIST[split:]]\n",
    "\n",
    "train_x, valid_x = split_dataset(dataset_df)\n",
    "print(\"{} examples in training, {} examples in testing.\".format(\n",
    "    len(train_x), len(valid_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the unique list of user sessions in the validation dataset. We assigned \n",
    "# `session_id` as the index of our feature engineered dataset. Hence fetching \n",
    "# the unique values in the index column will give us a list of users in the \n",
    "# validation set.\n",
    "VALID_USER_LIST = valid_x.index.unique()\n",
    "\n",
    "# Create a dataframe for storing the predictions of each question for all users\n",
    "# in the validation set.\n",
    "# For this, the required size of the data frame is: \n",
    "# (no: of users in validation set  x no of questions).\n",
    "# We will initialize all the predicted values in the data frame to zero.\n",
    "# The dataframe's index column is the user `session_id`s. \n",
    "prediction_df = pd.DataFrame(data=np.zeros((len(VALID_USER_LIST),18)), index=VALID_USER_LIST)\n",
    "\n",
    "# Create an empty dictionary to store the models created for each question.\n",
    "models = {}\n",
    "\n",
    "# Create an empty dictionary to store the evaluation score for each question.\n",
    "evaluation_dict ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get labels and data together\n",
    "# Iterate through questions 1 to 18 to train models for each question, evaluate\n",
    "# the trained model and store the predicted values.\n",
    "for q_no in range(1,19):\n",
    "\n",
    "    # Select level group for the question based on the q_no.\n",
    "    if q_no<=3: grp = '0-4'\n",
    "    elif q_no<=13: grp = '5-12'\n",
    "    elif q_no<=22: grp = '13-22'\n",
    "    print(\"### q_no\", q_no, \"grp\", grp)\n",
    "    \n",
    "        \n",
    "    # Filter the rows in the datasets based on the selected level group. \n",
    "    train_df = train_x.loc[train_x.level_group == grp]\n",
    "    train_users = train_df.index.values\n",
    "    valid_df = valid_x.loc[valid_x.level_group == grp]\n",
    "    valid_users = valid_df.index.values\n",
    "\n",
    "    # Select the labels for the related q_no.\n",
    "    train_labels = labels.loc[labels.q==q_no].set_index('session').loc[train_users]\n",
    "    valid_labels = labels.loc[labels.q==q_no].set_index('session').loc[valid_users]\n",
    "\n",
    "    # Add the label to the filtered datasets.\n",
    "    train_df[\"correct\"] = train_labels[\"correct\"]\n",
    "    valid_df[\"correct\"] = valid_labels[\"correct\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_rows(df, n_flatten=5, only_one=None, drop=None):\n",
    "    \"\"\"\n",
    "    Combines every n_flatten rows of a Pandas DataFrame into a new DataFrame, with each row containing the combined values from the n_flatten rows.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to combine.\n",
    "        n_flatten (int): The number of rows to be combined into a single row.\n",
    "        only_one (list): A list of column names to keep only the first occurrence of in the output DataFrame.\n",
    "        drop (list): A list of column names to drop from the input DataFrame before performing the calculation.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A new DataFrame containing one row for every n_flatten rows of the input DataFrame, with each row containing the combined values from the n_flatten rows.\n",
    "    \"\"\"\n",
    "    # Create a copy of the input DataFrame to modify\n",
    "    df = df.copy()\n",
    "\n",
    "    # Use value_counts() to get the count of each session_id\n",
    "    counts = df['session_id'].value_counts()\n",
    "\n",
    "    # Check if each group has the same number of rows\n",
    "    if (counts % n_flatten).any():\n",
    "        # Get the session_ids that need to be generated\n",
    "        need_generating = counts[counts < n_flatten].index.tolist()\n",
    "        num_generated_rows = 0\n",
    "        \n",
    "        # Loop through the session_ids that need to be generated\n",
    "        for session_id in need_generating:\n",
    "            # Generate a new row for this session_id\n",
    "            new_row = {\"session_id\": session_id}\n",
    "            for col in df.columns:\n",
    "                if col == \"session_id\":\n",
    "                    continue\n",
    "                elif df[col].dtype.name == \"category\":\n",
    "                    # Categorical column - set value to \"generated\"\n",
    "                    new_row[col] = \"generated\"\n",
    "                else:\n",
    "                    # Numeric column - set value to average of other values in column with the same level\n",
    "                    level_values = df.loc[df[\"session_id\"] == session_id, \"level\"].unique()\n",
    "                    for level in level_values:\n",
    "                        if level == \"generated\":\n",
    "                            continue\n",
    "                        other_values = df.loc[(df[\"session_id\"] == session_id) & (df[\"level\"] == level), col]\n",
    "                        if other_values.dtype.kind in 'biufc':\n",
    "                            new_value = other_values.mean()\n",
    "                            new_row[col] = new_value\n",
    "                            break\n",
    "            df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            num_generated_rows += 1\n",
    "\n",
    "        print(f\"Generated {num_generated_rows} rows with indices: {list(range(len(df) - num_generated_rows, len(df)))}\\n{new_row}\")\n",
    "\n",
    "\n",
    "    # Drop specified columns from input DataFrame\n",
    "    if drop:\n",
    "        df = df.drop(columns=drop)\n",
    "\n",
    "    # Determine the number of rows and columns in the input DataFrame\n",
    "    num_rows, num_cols = df.shape\n",
    "\n",
    "    # Determine the number of new rows in the output DataFrame\n",
    "    num_new_rows = num_rows // n_flatten\n",
    "\n",
    "    # Reshape the flattened values into a new array with the desired shape\n",
    "    values = df.values.flatten()\n",
    "    new_values = values.reshape(num_new_rows, n_flatten*num_cols)\n",
    "\n",
    "    # Create a new DataFrame from the reshaped values\n",
    "    new_df = pd.DataFrame(new_values, columns=[f\"{col}_{i}\" for i in range(1, n_flatten+1) for col in df.columns])\n",
    "\n",
    "    # Drop specified columns from output DataFrame\n",
    "    if only_one:\n",
    "        for col in only_one:\n",
    "            keep_col = f\"{col}_1\"\n",
    "            drop_cols = [f\"{col}_{i}\" for i in range(2, n_flatten+1)]\n",
    "            new_df = new_df.drop(columns=drop_cols)\n",
    "\n",
    "    return new_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "new_df = combine_rows(dataset_df_0_4,n_flatten= 5 ,drop= [\"level\"], only_one= [\"level_group\",\"music\", \"hq\", \"fullscreen\"])\n",
    "#print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13_22 = pd.read_csv(\"data\\processed\\df_13_22.csv\", index_col= 0, nrows= 20005)\n",
    "df_13_22 = df_13_22.reset_index(drop=True)\n",
    "ex = [\"level_group\",\"music\", \"hq\", \"fullscreen\"]\n",
    "drop = [\"level\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13_22 = combine_rows(df_13_22,n_flatten= 10 ,drop= drop, only_one= ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_rows(df, n_flatten=5, only_one=None, drop=None):\n",
    "    \"\"\"\n",
    "    Combines every n_flatten rows of a Pandas DataFrame into a new DataFrame, with each row containing the combined values from the n_flatten rows.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to combine.\n",
    "        n_flatten (int): The number of rows to be combined into a single row.\n",
    "        only_one (list): A list of column names to keep only the first occurrence of in the output DataFrame.\n",
    "        drop (list): A list of column names to drop from the input DataFrame before performing the calculation.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A new DataFrame containing one row for every n_flatten rows of the input DataFrame, with each row containing the combined values from the n_flatten rows.\n",
    "    \"\"\"\n",
    "    # Create a copy of the input DataFrame to modify\n",
    "    df = df.copy()\n",
    "\n",
    "    # Use value_counts() to get the count of each session_id\n",
    "    counts = df['session_id'].value_counts()\n",
    "\n",
    "    # Check if each group has the same number of rows\n",
    "    if (counts % n_flatten).any():\n",
    "        # Get the session_ids that need to be generated\n",
    "        need_generating = counts[counts < n_flatten].index.tolist()\n",
    "        num_generated_rows = 0\n",
    "        \n",
    "        # Loop through the session_ids that need to be generated\n",
    "        for session_id in need_generating:\n",
    "            # Generate new rows for this session_id\n",
    "            num_missing_rows = n_flatten - counts[session_id]\n",
    "            new_rows = []\n",
    "            for i in range(num_missing_rows):\n",
    "                new_row = {\"session_id\": session_id}\n",
    "                for col in df.columns:\n",
    "                    if col == \"session_id\":\n",
    "                        continue\n",
    "                    elif df[col].dtype.name == \"category\":\n",
    "                        # Categorical column - set value to \"generated\"\n",
    "                        new_row[col] = \"generated\"\n",
    "                    else:\n",
    "                        # Numeric column - set value to average of other values in column with the same level\n",
    "                        level_values = df.loc[df[\"session_id\"] == session_id, \"level\"].unique()\n",
    "                        for level in level_values:\n",
    "                            if level == \"generated\":\n",
    "                                continue\n",
    "                            other_values = df.loc[(df[\"session_id\"] == session_id) & (df[\"level\"] == level), col]\n",
    "                            if other_values.dtype.kind in 'biufc':\n",
    "                                new_value = other_values.mean()\n",
    "                                new_row[col] = new_value\n",
    "                                break\n",
    "                new_rows.append(new_row)\n",
    "            df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "            num_generated_rows += num_missing_rows\n",
    "\n",
    "        print(f\"Generated {num_generated_rows} rows with indices: {list(range(len(df) - num_generated_rows, len(df)))}\\n{new_rows}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Drop specified columns from input DataFrame\n",
    "    if drop:\n",
    "        df = df.drop(columns=drop)\n",
    "\n",
    "    # Determine the number of rows and columns in the input DataFrame\n",
    "    num_rows, num_cols = df.shape\n",
    "\n",
    "    # Determine the number of new rows in the output DataFrame\n",
    "    num_new_rows = (num_rows + num_generated_rows) // n_flatten\n",
    "\n",
    "    # Reshape the flattened values into a new array with the desired shape\n",
    "    values = df.values.flatten()\n",
    "    new_values = values.reshape(num_new_rows, n_flatten*num_cols)\n",
    "\n",
    "    # Create a new DataFrame from the reshaped values\n",
    "    new_df = pd.DataFrame(new_values, columns=[f\"{col}_{i}\" for i in range(1, n_flatten+1) for col in df.columns])\n",
    "\n",
    "    # Drop specified columns from output DataFrame\n",
    "    if only_one:\n",
    "        for col in only_one:\n",
    "            keep_col = f\"{col}_1\"\n",
    "            drop_cols = [f\"{col}_{i}\" for i in range(2, n_flatten+1)]\n",
    "            new_df = new_df.drop(columns=drop_cols)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_rows(df: pd.DataFrame, n_flatten: int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # Use value_counts() to get the count of each session_id\n",
    "    counts = df['session_id'].value_counts()\n",
    "\n",
    "    # Check if each group has the same number of rows\n",
    "    if (counts % n_flatten).any():\n",
    "        # Get the session_ids that need to be generated\n",
    "        need_generating = counts[counts < n_flatten].index.tolist()\n",
    "        num_generated_rows = 0\n",
    "        generated_sessions = []\n",
    "        \n",
    "        # Loop through the session_ids that need to be generated\n",
    "        for session_id in need_generating:\n",
    "            # Calculate the number of rows that need to be generated for this session_id\n",
    "            missing_rows = n_flatten - counts[session_id]\n",
    "            new_rows = []\n",
    "\n",
    "            # Generate new rows for this session_id\n",
    "            for i in range(missing_rows):\n",
    "                new_row = {\"session_id\": session_id}\n",
    "                for col in df.columns:\n",
    "                    if col == \"session_id\":\n",
    "                        continue\n",
    "                    elif df[col].dtype.name == \"category\":\n",
    "                        # Categorical column - set value to \"generated\"\n",
    "                        new_row[col] = \"generated\"\n",
    "                    else:\n",
    "                        # Numeric column - set value to average of other values in column with the same level\n",
    "                        level_values = df.loc[df[\"session_id\"] == session_id, \"level\"].unique()\n",
    "                        for level in level_values:\n",
    "                            if level == \"generated\":\n",
    "                                continue\n",
    "                            other_values = df.loc[(df[\"session_id\"] == session_id) & (df[\"level\"] == level), col]\n",
    "                            if other_values.dtype.kind in 'biufc':\n",
    "                                new_value = other_values.mean()\n",
    "                                new_row[col] = new_value\n",
    "                                break\n",
    "                new_rows.append(new_row)\n",
    "\n",
    "            # Add the new rows to the dataframe\n",
    "            df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "            num_generated_rows += missing_rows\n",
    "            generated_sessions.append({\"session_id\": session_id, \"num_rows\": missing_rows})\n",
    "\n",
    "        print(f\"Generated {num_generated_rows} rows with indices: {list(range(len(df) - num_generated_rows, len(df)))}\")\n",
    "        \n",
    "        # Create output dataframe 2\n",
    "        df2 = pd.DataFrame(generated_sessions)\n",
    "        df2 = df2.set_index(\"session_id\")\n",
    "        print(\"Generated rows per session id:\")\n",
    "        print(df2)\n",
    "        \n",
    "\n",
    "    return df, df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 55 rows with indices: [20005, 20006, 20007, 20008, 20009, 20010, 20011, 20012, 20013, 20014, 20015, 20016, 20017, 20018, 20019, 20020, 20021, 20022, 20023, 20024, 20025, 20026, 20027, 20028, 20029, 20030, 20031, 20032, 20033, 20034, 20035, 20036, 20037, 20038, 20039, 20040, 20041, 20042, 20043, 20044, 20045, 20046, 20047, 20048, 20049, 20050, 20051, 20052, 20053, 20054, 20055, 20056, 20057, 20058, 20059]\n",
      "Generated rows per session id:\n",
      "                   num_rows\n",
      "session_id                 \n",
      "20090316190523732         1\n",
      "20100214275616070         1\n",
      "20100007445515820         1\n",
      "20100411153285710         1\n",
      "20100510014844344         1\n",
      "20110212420906190         1\n",
      "20110012235136684         1\n",
      "20100118090535520         1\n",
      "20110212174595004         1\n",
      "20110010322671296         1\n",
      "20110209130717610         1\n",
      "20100116202427100         1\n",
      "20100620261406990         1\n",
      "20100319593860984         1\n",
      "20110209485141690         1\n",
      "20100318480867690         1\n",
      "20100115142089760         1\n",
      "20100318174525144         1\n",
      "20100115005512812         1\n",
      "20100523474474980         1\n",
      "20110210180909070         1\n",
      "20100610533703468         1\n",
      "20110210321880150         1\n",
      "20100312152757270         1\n",
      "20100311385101428         1\n",
      "20100210135106010         1\n",
      "20100110093663916         1\n",
      "20100014211946468         1\n",
      "20100211432649536         1\n",
      "20100413494435240         1\n",
      "20100018421509572         1\n",
      "20100211121053492         1\n",
      "20100020122406090         1\n",
      "20110116152879256         1\n",
      "20110213450098724         1\n",
      "20100510025822616         1\n",
      "20100209423871468         1\n",
      "20100309472366890         1\n",
      "20100512192183730         1\n",
      "20100310052382190         1\n",
      "20100310085206010         1\n",
      "20100513033741570         1\n",
      "20100310430520250         1\n",
      "20110215162449400         1\n",
      "20100415143631610         2\n",
      "20110219261342890         9\n"
     ]
    }
   ],
   "source": [
    "new_df, generated = generate_rows(df_13_22, n_flatten= 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm = new_df[\"session_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
