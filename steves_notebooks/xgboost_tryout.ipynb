{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports for the Data Preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import os\n",
    "import gc\n",
    "from typing import Tuple\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory:  n:\\MASTER_DS\\Code\\Kaggle_competition\\Kaggle-seminar\\student-performance\n"
     ]
    }
   ],
   "source": [
    "# get working directory\n",
    "wd = os.path.dirname(os.getcwd())\n",
    "# wd = \"N:\\MASTER_DS\\Code\\Kaggle_competition\\Kaggle-seminar\\student-performance\"\n",
    "os.chdir(wd)\n",
    "print(\"Working Directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader_steve import load_data, load_labels , load_all_X_y\n",
    "\n",
    "df, labels = load_all_X_y( data= \"flattened\", )\n",
    "keys = list(df.keys())\n",
    "\n",
    "dtypes_labels= {\n",
    "    'correct': np.uint8, \n",
    "    'q':np.uint8}\n",
    "\n",
    "lab_1 = pd.read_csv('data/processed/labels_q1-3.csv', dtype=dtypes_labels)\n",
    "lab_2 = pd.read_csv('data/processed/labels_q4-13.csv', dtype=dtypes_labels)\n",
    "lab_3= pd.read_csv('data/processed/labels_q14-18.csv', dtype=dtypes_labels)\n",
    "\n",
    "labels_dict = { \"0_4\": lab_1,\n",
    "        \"5_12\": lab_2,\n",
    "        \"13_22\": lab_3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(df.keys())\n",
    "labels_dict = {\n",
    "    \"0_4\": lab_1,\n",
    "    \"5_12\": lab_2,\n",
    "    \"13_22\": lab_3\n",
    "}\n",
    "\n",
    "# Create an empty dictionary to store the output DataFrames\n",
    "output_dict = {}\n",
    "\n",
    "# Iterate over each key in the original dictionary\n",
    "for key in keys:\n",
    "    train_data = df[key]\n",
    "    train_data = train_data.sort_values('session_id_1')\n",
    "    labels = labels_dict[key]\n",
    "    labels.rename(columns={'session': 'session_id_1'}, inplace=True)\n",
    "    labels = labels.sort_values('session_id_1')\n",
    "    merged_df = pd.merge(train_data, labels, on=\"session_id_1\", how=\"inner\")\n",
    "    merged_df = merged_df.drop(columns=[col for col in merged_df.columns if 'session' in col])\n",
    "    grouped_df = merged_df.sort_values('q')\n",
    "    \n",
    "    # Group the merged DataFrame by the \"q\" column\n",
    "    grouped_by_level = grouped_df.groupby('q')\n",
    "    \n",
    "    # Iterate over each unique level in the grouped DataFrame\n",
    "    for level, group_data in grouped_by_level:\n",
    "        # Create a unique key for the output dictionary\n",
    "        output_key = f\"{level}\"\n",
    "        # Drop the \"level_group\" and \"q\" columns from the group_data DataFrame\n",
    "        group_data = group_data.drop(columns=[\"level_group_1\", \"q\"])\n",
    "        # Store the group_data DataFrame in the output dictionary\n",
    "        output_dict[output_key] = group_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split ratio for train, test, and validation sets\n",
    "train_ratio = 0.7\n",
    "test_ratio = 0.2\n",
    "val_ratio = 0.1\n",
    "\n",
    "# Create dictionaries for train, test, and validation sets\n",
    "train_dict = {}\n",
    "test_dict = {}\n",
    "val_dict = {}\n",
    "\n",
    "# Iterate over each key in the output_dict\n",
    "for key, data in output_dict.items():\n",
    "    # Split data into train and remaining data (test + validation)\n",
    "    data_train, data_remaining = train_test_split(data, test_size=(test_ratio + val_ratio), random_state=42)\n",
    "    \n",
    "    # Split remaining data into test and validation\n",
    "    data_test, data_val = train_test_split(data_remaining, test_size=val_ratio / (test_ratio + val_ratio), random_state=42)\n",
    "    \n",
    "    # Store the data in respective dictionaries\n",
    "    train_dict[key] = data_train\n",
    "    test_dict[key] = data_test\n",
    "    val_dict[key] = data_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''check = train_dict[\"1\"]\n",
    "vars_to_keep = ['train_dict', 'test_dict', 'val_dict']\n",
    "all_vars = list(globals().keys())\n",
    "vars_to_drop = [var for var in all_vars if var not in vars_to_keep]\n",
    "\n",
    "for var in vars_to_drop:\n",
    "    del globals()[var]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm  # for progress tracking\n",
    "\n",
    "# Dictionary to store results\n",
    "results_dict = {}\n",
    "test_data_dict = {}\n",
    "#index to stop early\n",
    "index = 0\n",
    "limit = 1\n",
    "# Iterate over each key (model) in the output_dict\n",
    "for key, data in tqdm(output_dict.items(), desc='GridSearch Progress', total=len(output_dict)):\n",
    "    X = data.drop(\"correct\", axis=1)  # Features\n",
    "    y = data[\"correct\"]  # Labels\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    test_data_dict[key] = {\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test\n",
    "    }\n",
    "\n",
    "    # Define hyperparameter values to search for each model\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.1, 0.01, 0.001],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'gamma': [0, 1, 5],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'scale_pos_weight': [1, 5, 10]\n",
    "    }\n",
    "\n",
    "    # Create XGBoost classifier\n",
    "    model = xgb.XGBClassifier()\n",
    "\n",
    "    # Perform grid search with cross-validation for the current model\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=10, scoring='f1')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the grid search results for each hyperparameter combination\n",
    "    cv_results = grid_search.cv_results_\n",
    "\n",
    "    # Store results for the current model\n",
    "    results_dict[key] = {\n",
    "        \"cv_results\": cv_results,\n",
    "        \"best_params\": grid_search.best_params_,\n",
    "        \"best_score\": grid_search.best_score_,\n",
    "        \"f1_score\": f1_score(y_test, grid_search.predict(X_test))\n",
    "    }\n",
    "    index += 1\n",
    "    if index == limit:\n",
    "        break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from hyperopt import fmin, hp, tpe, Trials\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Define search space for hyperparameters\n",
    "param_space = {\n",
    "    'max_depth': hp.choice('max_depth', [3, 6, 9]),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.1)),\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300]),\n",
    "    'gamma': hp.choice('gamma', [0, 1, 5]),\n",
    "    'subsample': hp.uniform('subsample', 0.8, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.8, 1.0),\n",
    "    'scale_pos_weight': hp.choice('scale_pos_weight', [1, 5, 10])\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results_dict = {}\n",
    "test_data_dict = {}\n",
    "best_models = {}  # Dictionary to store best models\n",
    "\n",
    "# Define the objective function for hyperopt\n",
    "def objective(params):\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    score = f1_score(y_val, y_pred)\n",
    "    return -score  # maximize F1 score\n",
    "\n",
    "# Iterate over each key (model) in the train_dict\n",
    "for key, train_data in tqdm(train_dict.items()):\n",
    "    val_data = val_dict[key]\n",
    "    test_data = test_dict[key]\n",
    "\n",
    "    X_train = train_data.drop(\"correct\", axis=1)  # Features for training\n",
    "    y_train = train_data[\"correct\"]  # Labels for training\n",
    "\n",
    "    X_val = val_data.drop(\"correct\", axis=1)  # Features for validation\n",
    "    y_val = val_data[\"correct\"]  # Labels for validation\n",
    "    \n",
    "    X_test = test_data.drop(\"correct\", axis=1)  # Features for testing\n",
    "    y_test = test_data[\"correct\"]  # Labels for testing\n",
    "\n",
    "    # Run hyperparameter optimization using hyperopt\n",
    "    trials = Trials()\n",
    "    best_params = fmin(objective, param_space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "\n",
    "    # Fit the model with the best hyperparameters using train and validation data\n",
    "    model = xgb.XGBClassifier(**best_params)\n",
    "    X_train_val = pd.concat([X_train, X_val])\n",
    "    y_train_val = pd.concat([y_train, y_val])\n",
    "    model.fit(X_train_val, y_train_val)\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    best_score = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Store results for the current model\n",
    "    results_dict[key] = {\n",
    "        \"best_params\": best_params,\n",
    "        \"best_score\": best_score\n",
    "    }\n",
    "    \n",
    "    # Store the best model\n",
    "    best_models[key] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each key-value pair in results_dict\n",
    "for key, value in results_dict.items():\n",
    "    best_score = value[\"best_score\"]\n",
    "    print(f\"Model: {key}, Best Score: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.6818452380952381\n"
     ]
    }
   ],
   "source": [
    "#xgboost\n",
    "# import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Select the data for model training\n",
    "train_data = train_dict['10']\n",
    "X_train = train_data.drop(\"correct\", axis=1)\n",
    "y_train = train_data[\"correct\"]\n",
    "\n",
    "# Select the data for additional training (using validation set)\n",
    "val_data = val_dict['10']\n",
    "X_train_val = pd.concat([X_train, val_data.drop(\"correct\", axis=1)])\n",
    "y_train_val = pd.concat([y_train, val_data[\"correct\"]])\n",
    "\n",
    "# Select the data for testing\n",
    "test_data = test_dict['10']\n",
    "X_test = test_data.drop(\"correct\", axis=1)\n",
    "y_test = test_data[\"correct\"]\n",
    "\n",
    "# Define hyperparameters\n",
    "params = {\n",
    "    'max_depth': 9,\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 200,\n",
    "    'gamma': 1,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'scale_pos_weight': 5\n",
    "}\n",
    "\n",
    "# Train the model with the combined training and validation data\n",
    "model = xgb.XGBClassifier(**params)\n",
    "model.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.6765710889912345\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Select the data for model training\n",
    "train_data = train_dict['10']\n",
    "X_train = train_data.drop(\"correct\", axis=1)\n",
    "y_train = train_data[\"correct\"]\n",
    "\n",
    "# Select the data for additional training (using validation set)\n",
    "val_data = val_dict['10']\n",
    "X_val = val_data.drop(\"correct\", axis=1)\n",
    "y_val = val_data[\"correct\"]\n",
    "\n",
    "# Select the data for testing\n",
    "test_data = test_dict['10']\n",
    "X_test = test_data.drop(\"correct\", axis=1)\n",
    "y_test = test_data[\"correct\"]\n",
    "\n",
    "# Perform SVD-based dimensionality reduction\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "X_train_svd = svd.fit_transform(X_train)\n",
    "X_val_svd = svd.transform(X_val)\n",
    "X_test_svd = svd.transform(X_test)\n",
    "\n",
    "# Define hyperparameters\n",
    "params = {\n",
    "    'max_depth': 9,\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 200,\n",
    "    'gamma': 1,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'scale_pos_weight': 5\n",
    "}\n",
    "\n",
    "# Train the model with the combined training and validation data after dimensionality reduction\n",
    "model = xgb.XGBClassifier(**params)\n",
    "X_train_val_svd = pd.concat([pd.DataFrame(X_train_svd), pd.DataFrame(X_val_svd)])\n",
    "y_train_val = pd.concat([y_train, y_val])\n",
    "model.fit(X_train_val_svd, y_train_val)\n",
    "\n",
    "# Evaluate on the test set after dimensionality reduction\n",
    "y_pred = model.predict(X_test_svd)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_2096\\563449181.py:32: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=create_model, epochs=50, batch_size=32, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197/197 [==============================] - 0s 680us/step\n",
      "197/197 [==============================] - 0s 807us/step\n",
      "197/197 [==============================] - 0s 792us/step\n",
      "197/197 [==============================] - 0s 669us/step\n",
      "197/197 [==============================] - 0s 825us/step\n",
      "197/197 [==============================] - 0s 872us/step\n",
      "197/197 [==============================] - 0s 793us/step\n",
      "197/197 [==============================] - 0s 848us/step\n",
      "197/197 [==============================] - 0s 792us/step\n",
      "197/197 [==============================] - 0s 812us/step\n",
      "197/197 [==============================] - 0s 899us/step\n",
      "197/197 [==============================] - 0s 751us/step\n",
      "197/197 [==============================] - 0s 805us/step\n",
      "197/197 [==============================] - 0s 970us/step\n",
      "197/197 [==============================] - 0s 822us/step\n",
      "197/197 [==============================] - 0s 878us/step\n",
      "197/197 [==============================] - 0s 781us/step\n",
      "197/197 [==============================] - 0s 795us/step\n",
      "197/197 [==============================] - 0s 886us/step\n",
      "197/197 [==============================] - 0s 792us/step\n",
      "197/197 [==============================] - 0s 817us/step\n",
      "197/197 [==============================] - 0s 853us/step\n",
      "197/197 [==============================] - 0s 980us/step\n",
      "197/197 [==============================] - 0s 889us/step\n",
      "197/197 [==============================] - 0s 960us/step\n",
      "197/197 [==============================] - 0s 821us/step\n",
      "197/197 [==============================] - 0s 846us/step\n",
      "197/197 [==============================] - 0s 807us/step\n",
      "197/197 [==============================] - 0s 837us/step\n",
      "197/197 [==============================] - 0s 850us/step\n",
      "148/148 [==============================] - 0s 844us/step\n",
      "Best F1 Score: 0.6720600783200504\n",
      "F1 Score on Test Set: 0.6691145318457845\n",
      "Best Hyperparameters: {'optimizer': 'adam', 'learning_rate': 0.001, 'dropout_rate': 0.4, 'batch_size': 32}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "# Function to create the neural network model\n",
    "def create_model(optimizer='adam', learning_rate=0.001, dropout_rate=0.2):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = SGD(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(optimizer=opt, loss=BinaryCrossentropy())\n",
    "    return model\n",
    "\n",
    "# Create a KerasClassifier wrapper for Scikit-learn compatibility\n",
    "model = KerasClassifier(build_fn=create_model, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Define hyperparameters to search over\n",
    "param_grid = {\n",
    "    'optimizer': ['adam', 'sgd'],\n",
    "    'learning_rate': [0.01, 0.001],\n",
    "    'dropout_rate': [0.2, 0.3, 0.4],\n",
    "    'batch_size': [32, 64],\n",
    "}\n",
    "\n",
    "# Perform random search with cross-validation\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_grid, scoring='f1', n_iter=10, cv=3)\n",
    "random_search.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Get the best model and evaluate on the test set\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best F1 Score:\", random_search.best_score_)\n",
    "print(\"F1 Score on Test Set:\", f1)\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "590/590 [==============================] - 4s 6ms/step - loss: 173.1051\n",
      "Epoch 2/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 149.2191\n",
      "Epoch 3/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 139.4110\n",
      "Epoch 4/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 84.4823\n",
      "Epoch 5/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 111.0638\n",
      "Epoch 6/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 260.8571\n",
      "Epoch 7/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 94.1614\n",
      "Epoch 8/50\n",
      "590/590 [==============================] - 3s 5ms/step - loss: 142.6135\n",
      "Epoch 9/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 69.1689\n",
      "Epoch 10/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 104.0839\n",
      "Epoch 11/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 163.8286\n",
      "Epoch 12/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 113.4802\n",
      "Epoch 13/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 34.8535\n",
      "Epoch 14/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 40.8715\n",
      "Epoch 15/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 26.6101\n",
      "Epoch 16/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 19.9899\n",
      "Epoch 17/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 15.8622\n",
      "Epoch 18/50\n",
      "590/590 [==============================] - 3s 5ms/step - loss: 36.9534\n",
      "Epoch 19/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 5.2297\n",
      "Epoch 20/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 6.0747\n",
      "Epoch 21/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 3.3042\n",
      "Epoch 22/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 1.4727\n",
      "Epoch 23/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 8.3739\n",
      "Epoch 24/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 0.9377\n",
      "Epoch 25/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 1.2730\n",
      "Epoch 26/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 0.9307\n",
      "Epoch 27/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 5.1229\n",
      "Epoch 28/50\n",
      "590/590 [==============================] - 4s 6ms/step - loss: 0.9897\n",
      "Epoch 29/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 0.9155\n",
      "Epoch 30/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 0.9350\n",
      "Epoch 31/50\n",
      "590/590 [==============================] - 4s 6ms/step - loss: 0.7553\n",
      "Epoch 32/50\n",
      "590/590 [==============================] - 4s 6ms/step - loss: 1.7990\n",
      "Epoch 33/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 0.7111\n",
      "Epoch 34/50\n",
      "590/590 [==============================] - 4s 6ms/step - loss: 0.7656\n",
      "Epoch 35/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 0.8445\n",
      "Epoch 36/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 0.7057\n",
      "Epoch 37/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 0.7155\n",
      "Epoch 38/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 0.6938\n",
      "Epoch 39/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 0.6961\n",
      "Epoch 40/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 0.7075\n",
      "Epoch 41/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 0.9324\n",
      "Epoch 42/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 0.7670\n",
      "Epoch 43/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 0.6958\n",
      "Epoch 44/50\n",
      "590/590 [==============================] - 3s 5ms/step - loss: 0.6931\n",
      "Epoch 45/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 0.7357\n",
      "Epoch 46/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 0.7846\n",
      "Epoch 47/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 1.0875\n",
      "Epoch 48/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 0.6930\n",
      "Epoch 49/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 0.6986\n",
      "Epoch 50/50\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 0.6931\n",
      "148/148 [==============================] - 0s 2ms/step\n",
      "F1 Score (Bigger Model): 0.6691145318457845\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Select the data for model training\n",
    "train_data = train_dict['10']\n",
    "X_train = train_data.drop(\"correct\", axis=1)\n",
    "y_train = train_data[\"correct\"]\n",
    "\n",
    "# Select the data for additional training (using validation set)\n",
    "val_data = val_dict['10']\n",
    "X_val = val_data.drop(\"correct\", axis=1)\n",
    "y_val = val_data[\"correct\"]\n",
    "\n",
    "# Select the data for testing\n",
    "test_data = test_dict['10']\n",
    "X_test = test_data.drop(\"correct\", axis=1)\n",
    "y_test = test_data[\"correct\"]\n",
    "\n",
    "# Perform SVD-based dimensionality reduction\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "X_train_val = pd.concat([X_train, X_val])\n",
    "X_train_val_svd = svd.fit_transform(X_train_val)\n",
    "X_test_svd = svd.transform(X_test)\n",
    "\n",
    "# Define the bigger neural network model\n",
    "def create_bigger_model():\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(X_train_val_svd.shape[1],)),\n",
    "        Dropout(0.4),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=BinaryCrossentropy())\n",
    "    return model\n",
    "\n",
    "# Train the bigger neural network model with the combined training and validation data after dimensionality reduction\n",
    "bigger_model = create_bigger_model()\n",
    "bigger_model.fit(X_train_val_svd, pd.concat([y_train, y_val]), epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate on the test set after dimensionality reduction\n",
    "y_pred_bigger = bigger_model.predict(X_test_svd)\n",
    "y_pred_bigger = (y_pred_bigger > 0.5).astype(int)\n",
    "f1_bigger = f1_score(y_test, y_pred_bigger)\n",
    "\n",
    "print(\"F1 Score (Bigger Model):\", f1_bigger)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
