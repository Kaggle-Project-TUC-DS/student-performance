{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports for the Data Preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import os\n",
    "import gc\n",
    "from typing import Tuple\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory:  n:\\MASTER_DS\\Code\\Kaggle_competition\\Kaggle-seminar\\student-performance\n"
     ]
    }
   ],
   "source": [
    "# get working directory\n",
    "wd = os.path.dirname(os.getcwd())\n",
    "# wd = \"N:\\MASTER_DS\\Code\\Kaggle_competition\\Kaggle-seminar\\student-performance\"\n",
    "os.chdir(wd)\n",
    "print(\"Working Directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader_steve import load_data, load_labels , load_all_X_y\n",
    "\n",
    "df, labels = load_all_X_y( data= \"flattened\", )\n",
    "keys = list(df.keys())\n",
    "\n",
    "dtypes_labels= {\n",
    "    'correct': np.uint8, \n",
    "    'q':np.uint8}\n",
    "\n",
    "lab_1 = pd.read_csv('data/processed/labels_q1-3.csv', dtype=dtypes_labels)\n",
    "lab_2 = pd.read_csv('data/processed/labels_q4-13.csv', dtype=dtypes_labels)\n",
    "lab_3= pd.read_csv('data/processed/labels_q14-18.csv', dtype=dtypes_labels)\n",
    "\n",
    "labels_dict = { \"0_4\": lab_1,\n",
    "        \"5_12\": lab_2,\n",
    "        \"13_22\": lab_3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(df.keys())\n",
    "labels_dict = {\n",
    "    \"0_4\": lab_1,\n",
    "    \"5_12\": lab_2,\n",
    "    \"13_22\": lab_3\n",
    "}\n",
    "\n",
    "# Create an empty dictionary to store the output DataFrames\n",
    "output_dict = {}\n",
    "\n",
    "# Iterate over each key in the original dictionary\n",
    "for key in keys:\n",
    "    train_data = df[key]\n",
    "    train_data = train_data.sort_values('session_id_1')\n",
    "    labels = labels_dict[key]\n",
    "    labels.rename(columns={'session': 'session_id_1'}, inplace=True)\n",
    "    labels = labels.sort_values('session_id_1')\n",
    "    merged_df = pd.merge(train_data, labels, on=\"session_id_1\", how=\"inner\")\n",
    "    merged_df = merged_df.drop(columns=[col for col in merged_df.columns if 'session' in col])\n",
    "    grouped_df = merged_df.sort_values('q')\n",
    "    \n",
    "    # Group the merged DataFrame by the \"q\" column\n",
    "    grouped_by_level = grouped_df.groupby('q')\n",
    "    \n",
    "    # Iterate over each unique level in the grouped DataFrame\n",
    "    for level, group_data in grouped_by_level:\n",
    "        # Create a unique key for the output dictionary\n",
    "        output_key = f\"{level}\"\n",
    "        # Drop the \"level_group\" and \"q\" columns from the group_data DataFrame\n",
    "        group_data = group_data.drop(columns=[\"level_group_1\", \"q\"])\n",
    "        # Store the group_data DataFrame in the output dictionary\n",
    "        output_dict[output_key] = group_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split ratio for train, test, and validation sets\n",
    "train_ratio = 0.7\n",
    "test_ratio = 0.2\n",
    "val_ratio = 0.1\n",
    "\n",
    "# Create dictionaries for train, test, and validation sets\n",
    "train_dict = {}\n",
    "test_dict = {}\n",
    "val_dict = {}\n",
    "\n",
    "# Iterate over each key in the output_dict\n",
    "for key, data in output_dict.items():\n",
    "    # Split data into train and remaining data (test + validation)\n",
    "    data_train, data_remaining = train_test_split(data, test_size=(test_ratio + val_ratio), random_state=42)\n",
    "    \n",
    "    # Split remaining data into test and validation\n",
    "    data_test, data_val = train_test_split(data_remaining, test_size=val_ratio / (test_ratio + val_ratio), random_state=42)\n",
    "    \n",
    "    # Store the data in respective dictionaries\n",
    "    train_dict[key] = data_train\n",
    "    test_dict[key] = data_test\n",
    "    val_dict[key] = data_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = train_dict[\"1\"]\n",
    "vars_to_keep = ['train_dict', 'test_dict', 'val_dict']\n",
    "all_vars = list(globals().keys())\n",
    "vars_to_drop = [var for var in all_vars if var not in vars_to_keep]\n",
    "\n",
    "for var in vars_to_drop:\n",
    "    del globals()[var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm  # for progress tracking\n",
    "\n",
    "# Dictionary to store results\n",
    "results_dict = {}\n",
    "test_data_dict = {}\n",
    "#index to stop early\n",
    "index = 0\n",
    "limit = 1\n",
    "# Iterate over each key (model) in the output_dict\n",
    "for key, data in tqdm(output_dict.items(), desc='GridSearch Progress', total=len(output_dict)):\n",
    "    X = data.drop(\"correct\", axis=1)  # Features\n",
    "    y = data[\"correct\"]  # Labels\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    test_data_dict[key] = {\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test\n",
    "    }\n",
    "\n",
    "    # Define hyperparameter values to search for each model\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.1, 0.01, 0.001],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'gamma': [0, 1, 5],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'scale_pos_weight': [1, 5, 10]\n",
    "    }\n",
    "\n",
    "    # Create XGBoost classifier\n",
    "    model = xgb.XGBClassifier()\n",
    "\n",
    "    # Perform grid search with cross-validation for the current model\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=10, scoring='f1')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the grid search results for each hyperparameter combination\n",
    "    cv_results = grid_search.cv_results_\n",
    "\n",
    "    # Store results for the current model\n",
    "    results_dict[key] = {\n",
    "        \"cv_results\": cv_results,\n",
    "        \"best_params\": grid_search.best_params_,\n",
    "        \"best_score\": grid_search.best_score_,\n",
    "        \"f1_score\": f1_score(y_test, grid_search.predict(X_test))\n",
    "    }\n",
    "    index += 1\n",
    "    if index == limit:\n",
    "        break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [14:53<00:00,  8.93s/trial, best loss: -0.8436421555614548]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1/18 [14:53<4:13:04, 893.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 100/100 [07:29<00:00,  4.50s/trial, best loss: -0.9899292907649453]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2/18 [22:22<2:48:36, 632.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 100/100 [10:54<00:00,  6.54s/trial, best loss: -0.969590899146795]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 3/18 [33:17<2:40:34, 642.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 100/100 [14:32<00:00,  8.72s/trial, best loss: -0.8942261038330908]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 4/18 [47:49<2:51:02, 733.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:02<00:00, 10.23s/trial, best loss: -0.7262826024648895]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 5/18 [1:04:52<3:01:28, 837.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 100/100 [08:50<00:00,  5.31s/trial, best loss: -0.8793315310887196]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 6/18 [1:13:42<2:26:38, 733.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 100/100 [15:19<00:00,  9.20s/trial, best loss: -0.8393166625402327]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 7/18 [1:29:02<2:25:37, 794.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 100/100 [14:03<00:00,  8.43s/trial, best loss: -0.7641633728590251]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 8/18 [1:43:06<2:14:59, 809.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 100/100 [09:57<00:00,  5.97s/trial, best loss: -0.8443775100401607]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 8/18 [1:53:03<2:21:19, 847.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[20:20:53] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\tree\\updater_colmaker.cc:177: Check failed: param_.max_depth > 0 (0 vs. 0) : exact tree method doesn't support unlimited depth.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mn:\\MASTER_DS\\Code\\Kaggle_competition\\Kaggle-seminar\\student-performance\\steves_notebooks\\xgboost_tryout.ipynb Cell 8\u001b[0m in \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/steves_notebooks/xgboost_tryout.ipynb#X12sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m X_train_val \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([X_train, X_val])\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/steves_notebooks/xgboost_tryout.ipynb#X12sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m y_train_val \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([y_train, y_val])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/steves_notebooks/xgboost_tryout.ipynb#X12sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m model\u001b[39m.\u001b[39mfit(X_train_val, y_train_val)\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/steves_notebooks/xgboost_tryout.ipynb#X12sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# Evaluate on the test set\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MASTER_DS/Code/Kaggle_competition/Kaggle-seminar/student-performance/steves_notebooks/xgboost_tryout.ipynb#X12sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mn:\\Python\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mn:\\Python\\lib\\site-packages\\xgboost\\sklearn.py:1490\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1462\u001b[0m (\n\u001b[0;32m   1463\u001b[0m     model,\n\u001b[0;32m   1464\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1470\u001b[0m )\n\u001b[0;32m   1471\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1472\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[0;32m   1473\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1487\u001b[0m     feature_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types,\n\u001b[0;32m   1488\u001b[0m )\n\u001b[1;32m-> 1490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m   1491\u001b[0m     params,\n\u001b[0;32m   1492\u001b[0m     train_dmatrix,\n\u001b[0;32m   1493\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[0;32m   1494\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[0;32m   1495\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[0;32m   1496\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[0;32m   1497\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[0;32m   1498\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m   1499\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1500\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1501\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1502\u001b[0m )\n\u001b[0;32m   1504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[0;32m   1505\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mn:\\Python\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mn:\\Python\\lib\\site-packages\\xgboost\\training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    184\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[0;32m    186\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    187\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mn:\\Python\\lib\\site-packages\\xgboost\\core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[0;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1918\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m   1919\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[0;32m   1920\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[0;32m   1921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1922\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mn:\\Python\\lib\\site-packages\\xgboost\\core.py:279\u001b[0m, in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \n\u001b[0;32m    270\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [20:20:53] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\tree\\updater_colmaker.cc:177: Check failed: param_.max_depth > 0 (0 vs. 0) : exact tree method doesn't support unlimited depth."
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from hyperopt import fmin, hp, tpe, Trials\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Define search space for hyperparameters\n",
    "param_space = {\n",
    "    'max_depth': hp.choice('max_depth', [3, 6, 9]),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.1)),\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300]),\n",
    "    'gamma': hp.choice('gamma', [0, 1, 5]),\n",
    "    'subsample': hp.uniform('subsample', 0.8, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.8, 1.0),\n",
    "    'scale_pos_weight': hp.choice('scale_pos_weight', [1, 5, 10])\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results_dict = {}\n",
    "test_data_dict = {}\n",
    "best_models = {}  # Dictionary to store best models\n",
    "\n",
    "# Define the objective function for hyperopt\n",
    "def objective(params):\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    score = f1_score(y_val, y_pred)\n",
    "    return -score  # maximize F1 score\n",
    "\n",
    "# Iterate over each key (model) in the train_dict\n",
    "for key, train_data in tqdm(train_dict.items()):\n",
    "    val_data = val_dict[key]\n",
    "    test_data = test_dict[key]\n",
    "\n",
    "    X_train = train_data.drop(\"correct\", axis=1)  # Features for training\n",
    "    y_train = train_data[\"correct\"]  # Labels for training\n",
    "\n",
    "    X_val = val_data.drop(\"correct\", axis=1)  # Features for validation\n",
    "    y_val = val_data[\"correct\"]  # Labels for validation\n",
    "    \n",
    "    X_test = test_data.drop(\"correct\", axis=1)  # Features for testing\n",
    "    y_test = test_data[\"correct\"]  # Labels for testing\n",
    "\n",
    "    # Run hyperparameter optimization using hyperopt\n",
    "    trials = Trials()\n",
    "    best_params = fmin(objective, param_space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "\n",
    "    # Fit the model with the best hyperparameters using train and validation data\n",
    "    model = xgb.XGBClassifier(**best_params)\n",
    "    X_train_val = pd.concat([X_train, X_val])\n",
    "    y_train_val = pd.concat([y_train, y_val])\n",
    "    model.fit(X_train_val, y_train_val)\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    best_score = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Store results for the current model\n",
    "    results_dict[key] = {\n",
    "        \"best_params\": best_params,\n",
    "        \"best_score\": best_score\n",
    "    }\n",
    "    \n",
    "    # Store the best model\n",
    "    best_models[key] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 1, Best Score: 0.0\n",
      "Model: 2, Best Score: 0.0\n",
      "Model: 3, Best Score: 0.0\n",
      "Model: 4, Best Score: 0.0\n",
      "Model: 5, Best Score: 0.7024438573315721\n",
      "Model: 6, Best Score: 0.0\n",
      "Model: 7, Best Score: 0.8466527964753396\n",
      "Model: 8, Best Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each key-value pair in results_dict\n",
    "for key, value in results_dict.items():\n",
    "    best_score = value[\"best_score\"]\n",
    "    print(f\"Model: {key}, Best Score: {best_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
